{
  "hash": "f6e2b41d6ccaae9f4e46bb1ada82312d",
  "result": {
    "engine": "knitr",
    "markdown": "# MMs and AMCEs with OLS\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\nlibrary(marginaleffects)\nlibrary(parameters)\nlibrary(tinytable)\nlibrary(scales)\nlibrary(ggforce)\n\nstickers <- readRDS(\"data/processed_data/study_5_sticker.rds\")\n```\n:::\n\n\n\n## Model\n\nWhen estimating causal effects, the main estimand of interest is an average treatment effect, or $E(Y \\mid X)$. In political science and economics, analysts typically rely on an ordinary least squares (OLS) estimator, or a linear probability model (LPM), where the $\\beta$ term in a linear regression model (or a partial derivative or marginal effect, if interaction terms are involved) represents the average effect of the treatment on the outcome. For those trained in predictive modeling or for Bayesians who seek out distributional families that reflect the underlying data generating process of the outcome variable, using LPMs on a binary (or multinomial) outcome can feel wrong. However, @Gomila:2021 demonstrates that in experiments with binary outcomes, LPMs are typically more consistent and unbiased than logistic regression estimators.\n\nThis means that analyzing conjoint data can be as simple as a basic linear model with `lm()`.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel_ols <- lm(\n  choice ~ price + packaging + flavor,\n  data = stickers\n)\n\nmodel_parameters(model_ols, verbose = FALSE)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nParameter                     | Coefficient |   SE |         95% CI | t(7075) |      p\n--------------------------------------------------------------------------------------\n(Intercept)                   |        0.80 | 0.01 | [ 0.78,  0.82] |   69.39 | < .001\nprice [$3]                    |       -0.14 | 0.01 | [-0.17, -0.12] |  -11.38 | < .001\nprice [$4]                    |       -0.32 | 0.01 | [-0.34, -0.29] |  -25.23 | < .001\npackaging [Plastic + sticker] |        0.11 | 0.01 | [ 0.09,  0.14] |   11.15 | < .001\nflavor [Nuts]                 |       -0.41 | 0.01 | [-0.43, -0.39] |  -39.46 | < .001\n```\n\n\n:::\n:::\n\n\nWhile it is tempting (and possible) to determine the marginal means and causal effects from these raw regression coefficients—i.e., the intercept represents the average probability of selecting a \\$2 granola bar when all other characteristics are set to their reference values—it is more advisable to use the post-estimation functions from the {marginaleffects} package to calculate average predictions and comparisons. {marginaleffects} can provide probability-scale averages and contrasts, can calculate marginal means and effects across a balanced grid of attributed levels, and can adjust the estimated standard errors to account for repeated respondents.\n\n## Marginal means\n\nMarginal means represent the probability-scale fitted values from the model, calculated across a balanced reference grid of all possible combinations of feature levels. These predictions are then marginalized or averaged across features of interest.\n\nIn the case of the sticker experiment, there are 12 possible combinations of price, packaging, and flavor:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfeature_grid <- stickers |> \n  tidyr::expand(price, packaging, flavor)\ntt(feature_grid)\n```\n\n::: {.cell-output-display}\n\\begin{table}\n\\centering\n\\begin{tblr}[         %% tabularray outer open\n]                     %% tabularray outer close\n{                     %% tabularray inner open\ncolspec={Q[]Q[]Q[]},\n}                     %% tabularray inner close\n\\toprule\nprice & packaging & flavor \\\\ \\midrule %% TinyTableHeader\n$2 & Plastic + paper & Chocolate \\\\\n$2 & Plastic + paper & Nuts \\\\\n$2 & Plastic + sticker & Chocolate \\\\\n$2 & Plastic + sticker & Nuts \\\\\n$3 & Plastic + paper & Chocolate \\\\\n$3 & Plastic + paper & Nuts \\\\\n$3 & Plastic + sticker & Chocolate \\\\\n$3 & Plastic + sticker & Nuts \\\\\n$4 & Plastic + paper & Chocolate \\\\\n$4 & Plastic + paper & Nuts \\\\\n$4 & Plastic + sticker & Chocolate \\\\\n$4 & Plastic + sticker & Nuts \\\\\n\\bottomrule\n\\end{tblr}\n\\end{table}\n:::\n:::\n\n\nWe can feed each row of this balanced grid into the model to generate 12 predicted values:\n\n\n::: {.cell}\n\n```{.r .cell-code}\npredictions(model_ols, newdata = feature_grid)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n Estimate Std. Error     z Pr(>|z|)     S  2.5 % 97.5 %\n   0.8003     0.0115 69.39   <0.001   Inf 0.7777 0.8229\n   0.3933     0.0115 34.10   <0.001 844.2 0.3707 0.4159\n   0.9153     0.0115 79.39   <0.001   Inf 0.8927 0.9379\n   0.5082     0.0115 44.07   <0.001   Inf 0.4856 0.5308\n   0.6565     0.0115 56.91   <0.001   Inf 0.6339 0.6791\n   0.2495     0.0115 21.63   <0.001 342.4 0.2269 0.2721\n   0.7715     0.0115 66.88   <0.001   Inf 0.7489 0.7941\n   0.3645     0.0115 31.59   <0.001 725.3 0.3419 0.3871\n   0.4815     0.0115 41.73   <0.001   Inf 0.4589 0.5042\n   0.0745     0.0115  6.46   <0.001  33.2 0.0519 0.0971\n   0.5965     0.0115 51.71   <0.001   Inf 0.5739 0.6191\n   0.1895     0.0115 16.43   <0.001 199.2 0.1669 0.2121\n\nType: response\n```\n\n\n:::\n:::\n\n\nFinally, we can marginalize or average these predicted values across features of interest. For instance, to find the marginal means for the two packaging conditions, we can calculate the group averages for the two types of packaging:\n\n\n::: {.cell}\n\n```{.r .cell-code}\npredictions(model_ols, newdata = feature_grid) |> \n  group_by(packaging) |> \n  summarize(avg = mean(estimate))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 2 x 2\n  packaging           avg\n  <fct>             <dbl>\n1 Plastic + paper   0.443\n2 Plastic + sticker 0.558\n```\n\n\n:::\n:::\n\n\nManually creating a balanced reference grid and using `group_by()` and `summarize()` is useful for understanding the intuition behind finding estimated marginal means, but in practice it is better to use `avg_predictions()` from {marginaleffects}, which (1) creates the balanced grid automatically, (2) provides standard errors and other estimates of uncertainty, and (3) can adjust the standard errors to account for repeated respondents:\n\n\n::: {.cell}\n\n```{.r .cell-code}\navg_predictions(\n  model_ols,\n  newdata = \"balanced\",\n  by = \"packaging\",\n  vcov = ~resp_id\n)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n         packaging Estimate Std. Error    z Pr(>|z|)   S 2.5 % 97.5 %\n Plastic + paper      0.443    0.00868 51.0   <0.001 Inf 0.426  0.460\n Plastic + sticker    0.558    0.00881 63.3   <0.001 Inf 0.540  0.575\n\nType: response\n```\n\n\n:::\n:::\n\n\nWe can calculate the marginal means individually for each conjoint feature, then combine them all into one large data frame for plotting and table-making.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmm_ols <- c(\"price\", \"packaging\", \"flavor\") |>\n  set_names(str_to_title) |>\n  map(\\(x) {\n    avg_predictions(\n      model_ols,\n      newdata = \"balanced\",\n      by = x,\n      vcov = ~resp_id\n    ) |>\n      rename(attribute = all_of(x))\n  }) |>\n  list_rbind(names_to = \"feature\")\n```\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(\n  mm_ols,\n  aes(x = estimate, y = fct_rev(attribute), color = feature)\n) +\n  geom_vline(xintercept = 0.5) +\n  geom_pointrange(aes(xmin = conf.low, xmax = conf.high)) +\n  scale_x_continuous(labels = label_percent()) +\n  guides(color = \"none\") +\n  facet_col(vars(feature), scales = \"free_y\", space = \"free\")\n```\n\n::: {.cell-output-display}\n![](mms-amces-ols_files/figure-pdf/unnamed-chunk-7-1.pdf){fig-pos='H'}\n:::\n:::\n\n\n## Average marginal component effects (AMCEs)\n\n\n::: {.cell}\n\n```{.r .cell-code}\namce_ols <- avg_comparisons(\n  model_ols,\n  newdata = \"balanced\",\n  vcov = ~resp_id\n)\namce_ols\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n      Term                            Contrast Estimate Std. Error      z\n flavor    Nuts - Chocolate                      -0.407     0.0226 -18.04\n packaging Plastic + sticker - Plastic + paper    0.115     0.0174   6.61\n price     $3 - $2                               -0.144     0.0131 -10.99\n price     $4 - $2                               -0.319     0.0187 -17.09\n Pr(>|z|)     S   2.5 % 97.5 %\n   <0.001 239.2 -0.4513 -0.363\n   <0.001  34.6  0.0809  0.149\n   <0.001  90.9 -0.1694 -0.118\n   <0.001 215.1 -0.3553 -0.282\n\nType: response\n```\n\n\n:::\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\namces_split <- amce_ols |>\n  separate_wider_delim(\n    contrast, \n    delim = \" - \", \n    names = c(\"attribute\", \"reference_level\")\n  )\n\nreference_categories <- amces_split |>\n  distinct(term, reference_level) |>\n  rename(attribute = reference_level) |>\n  mutate(estimate = 0, conf.low = 0, conf.high = 0)\n\namces_split |>\n  bind_rows(reference_categories) |> \n  ggplot(aes(x = estimate, y = fct_rev(attribute), color = term)) +\n  geom_vline(xintercept = 0) +\n  geom_pointrange(aes(xmin = conf.low, xmax = conf.high)) +\n  scale_x_continuous(labels = label_percent()) +\n  guides(color = \"none\") +\n  facet_col(vars(term), scales = \"free_y\", space = \"free\")\n```\n\n::: {.cell-output-display}\n![](mms-amces-ols_files/figure-pdf/unnamed-chunk-9-1.pdf){fig-pos='H'}\n:::\n:::\n\n\n## Average Feature Choice Probability (AFCP)\n\nMaybe?\n",
    "supporting": [
      "mms-amces-ols_files/figure-pdf"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {
      "knitr": [
        "{\"type\":\"list\",\"attributes\":{\"knit_meta_id\":{\"type\":\"character\",\"attributes\":{},\"value\":[\"\",\"\"]}},\"value\":[{\"type\":\"list\",\"attributes\":{\"names\":{\"type\":\"character\",\"attributes\":{},\"value\":[\"name\",\"options\",\"extra_lines\"]},\"class\":{\"type\":\"character\",\"attributes\":{},\"value\":[\"latex_dependency\"]}},\"value\":[{\"type\":\"character\",\"attributes\":{},\"value\":[\"float\"]},{\"type\":\"NULL\"},{\"type\":\"NULL\"}]},{\"type\":\"list\",\"attributes\":{\"names\":{\"type\":\"character\",\"attributes\":{},\"value\":[\"name\",\"options\",\"extra_lines\"]},\"class\":{\"type\":\"character\",\"attributes\":{},\"value\":[\"latex_dependency\"]}},\"value\":[{\"type\":\"character\",\"attributes\":{},\"value\":[\"tabularray\"]},{\"type\":\"NULL\"},{\"type\":\"character\",\"attributes\":{},\"value\":[\"\\\\usepackage[normalem]{ulem}\",\"\\\\usepackage{graphicx}\",\"\\\\usepackage{rotating}\",\"\\\\UseTblrLibrary{booktabs}\",\"\\\\UseTblrLibrary{siunitx}\",\"\\\\NewTableCommand{\\\\tinytableDefineColor}[3]{\\\\definecolor{#1}{#2}{#3}}\",\"\\\\newcommand{\\\\tinytableTabularrayUnderline}[1]{\\\\underline{#1}}\",\"\\\\newcommand{\\\\tinytableTabularrayStrikeout}[1]{\\\\sout{#1}}\"]}]}]}"
      ]
    },
    "preserve": null,
    "postProcess": false
  }
}