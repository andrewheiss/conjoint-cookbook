{
  "hash": "adea637ddc86b48ef685256de499c70b",
  "result": {
    "engine": "knitr",
    "markdown": "# MMs and AMCEs with frequentist multinomial regression\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\nlibrary(mlogit)\nlibrary(marginaleffects)\nlibrary(parameters)\nlibrary(tinytable)\nlibrary(scales)\nlibrary(ggforce)\n\nstickers <- readRDS(\"data/processed_data/study_5_sticker.rds\")\n\nlabel_pp <- scales::label_number(\n  accuracy = 1, scale = 100, suffix = \" pp.\", style_negative = \"minus\"\n)\n```\n:::\n\n\n## Model\n\nIt is also possible to use a multinomial logistic regression model that matches the distribution of the choice outcome variable. This can be done with a variety of R packages, including {mlogit}, {mclogit}, {logitr}, and {nnet}. Each behave slightly differently, requiring modifications to the data structure or needing additional post-processing work.\n\n| Package | Data restructuring | Supports {marginaleffects}? | Allows for random effects? |\n|------------------|------------------|------------------|------------------|\n| {mlogit} | Requires an indexed data frame made with `dfidx()` | No | Yes, with `rpar` argument |\n| {mclogit} | Requires a unique choice ID index | Yes | Yes, with `random` argument |\n| {logitr} | Requires a unique choice ID index | No | Yes, with `randPars` argument |\n| {nnet} | None | Yes | No |\n\n\n\n::: {.panel-tabset}\n### {mlogit}\n\n{mlogit} needs to work with an indexed data frame (created with `dfidx()`) that keeps track of the nested choices within respondents:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nstickers_indexed <- stickers |> \n  group_by(resp_id, question) |> \n  mutate(choice_id = cur_group_id()) |> \n  ungroup() |> \n  as.data.frame() |>  # mlogit() complains and breaks when working with tibbles :(\n  dfidx(\n    idx = list(c(\"choice_id\", \"resp_id\"), \"alt\"),\n    choice = \"choice\"\n  )\n```\n:::\n\n\nIt then uses R's standard formula syntax to define the model:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel_mlogit <- mlogit(\n  choice ~ price + packaging + flavor | 0,\n  data = stickers_indexed\n)\n\nmodel_parameters(model_mlogit, verbose = FALSE)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nParameter                     | Log-Odds |   SE |         95% CI |      z |      p\n----------------------------------------------------------------------------------\nprice [$3]                    |    -0.74 | 0.07 | [-0.87, -0.60] | -10.79 | < .001\nprice [$4]                    |    -1.61 | 0.08 | [-1.76, -1.46] | -21.26 | < .001\npackaging [Plastic + sticker] |     0.54 | 0.05 | [ 0.44,  0.64] |  10.24 | < .001\nflavor [Nuts]                 |    -1.69 | 0.06 | [-1.81, -1.58] | -27.81 | < .001\n```\n\n\n:::\n:::\n\n\n\n### {mclogit}\n\n{mclogit} does not need an official indexed data frame, but it does need a unique identifer for each possible choice:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nstickers_mclogit <- stickers |> \n  group_by(resp_id, question) |> \n  mutate(choice_id = cur_group_id()) |> \n  ungroup()\n\nhead(stickers_mclogit)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 6 x 12\n  condition resp_id question   alt gender   age chosen_alt price packaging      \n  <chr>       <dbl>    <dbl> <dbl> <chr>  <dbl>      <dbl> <fct> <fct>          \n1 Sticker         4        1     1 Female    19          1 $3    Plastic + stic~\n2 Sticker         4        1     2 Female    19          1 $2    Plastic + paper\n3 Sticker         4        2     1 Female    19          2 $3    Plastic + stic~\n4 Sticker         4        2     2 Female    19          2 $2    Plastic + paper\n5 Sticker         4        3     1 Female    19          1 $4    Plastic + paper\n6 Sticker         4        3     2 Female    19          1 $2    Plastic + stic~\n# i 3 more variables: flavor <fct>, choice <dbl>, choice_id <int>\n```\n\n\n:::\n:::\n\n\nWe can again use R's formula syntax, but we need to specify two parts in the left-hand side: the binary choice variable and the set of choices it is nested in:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(mclogit)\n\nmodel_mclogit <- mclogit(\n  choice | choice_id ~ price + packaging + flavor,\n  data = stickers_mclogit\n)\n```\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel_parameters(model_mclogit, verbose = FALSE)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nParameter                     | Log-Odds |   SE |         95% CI |      z |      p\n----------------------------------------------------------------------------------\nprice [$3]                    |    -0.74 | 0.07 | [-0.87, -0.60] | -10.79 | < .001\nprice [$4]                    |    -1.61 | 0.08 | [-1.76, -1.46] | -21.26 | < .001\npackaging [Plastic + sticker] |     0.54 | 0.05 | [ 0.44,  0.64] |  10.24 | < .001\nflavor [Nuts]                 |    -1.69 | 0.06 | [-1.81, -1.58] | -27.81 | < .001\n```\n\n\n:::\n:::\n\n\n### {logitr}\n\n{logitr} \n\n\n::: {.cell}\n\n```{.r .cell-code}\nstickers_logitr <- stickers |> \n  group_by(resp_id, question) |> \n  mutate(choice_id = cur_group_id()) |> \n  ungroup()\n\nhead(stickers_logitr)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 6 x 12\n  condition resp_id question   alt gender   age chosen_alt price packaging      \n  <chr>       <dbl>    <dbl> <dbl> <chr>  <dbl>      <dbl> <fct> <fct>          \n1 Sticker         4        1     1 Female    19          1 $3    Plastic + stic~\n2 Sticker         4        1     2 Female    19          1 $2    Plastic + paper\n3 Sticker         4        2     1 Female    19          2 $3    Plastic + stic~\n4 Sticker         4        2     2 Female    19          2 $2    Plastic + paper\n5 Sticker         4        3     1 Female    19          1 $4    Plastic + paper\n6 Sticker         4        3     2 Female    19          1 $2    Plastic + stic~\n# i 3 more variables: flavor <fct>, choice <dbl>, choice_id <int>\n```\n\n\n:::\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(logitr)\n\nmodel_logitr <- logitr(\n  data = stickers_logitr,\n  outcome = \"choice\",\n  obsID = \"choice_id\",\n  pars = c(\"price\", \"packaging\", \"flavor\")\n)\n```\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel_parameters(model_logitr, verbose = FALSE)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nParameter                     | Log-Odds |   SE |         95% CI |      z |      p\n----------------------------------------------------------------------------------\nprice [$3]                    |    -0.74 | 0.07 | [-0.87, -0.60] | -10.79 | < .001\nprice [$4]                    |    -1.61 | 0.08 | [-1.76, -1.46] | -21.26 | < .001\npackaging [Plastic + sticker] |     0.54 | 0.05 | [ 0.44,  0.64] |  10.24 | < .001\nflavor [Nuts]                 |    -1.69 | 0.06 | [-1.81, -1.58] | -27.81 | < .001\n```\n\n\n:::\n:::\n\n\n### {nnet}\n\n{nnet} requires no indexing or question identifiers, which also means that it pools all the observations together and disregards the nested structure of the data.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(nnet)\n\nmodel_nnet <- multinom(\n  choice ~ price + packaging + flavor, \n  data = stickers\n)\n```\n:::\n\n\n:::\n\n{mlogit} is the oldest and most commonly used multinomial regression package and is the basis for many conjoint textbooks [@Feit], so I'll illustrate how to use it to calculate MMs and AMCEs. {marginaleffects} does not support {mlogit} models because of the idiosyncracies its prediction functions, so the process requires a little manual work.\n\n\n\n## Marginal means\n\nTo calculate marginal means, we need to generate predicted probabilities across a balanced grid of all conjoint features. This is a little trickier to do with multinomial {mlogit} models, though. {mlogit}'s `predict()` function requires that any new data passed to it include a row for each alternative (`alt` in the original data), since it will generate predictions for each alternative.\n\nFor example, if we only feed one combination of conjoint features to `predict()`, we'll get an error:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nnewdata_example_bad <- stickers |> \n  slice(1) |> \n  select(price, packaging, flavor)\nnewdata_example_bad\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 1 x 3\n  price packaging         flavor   \n  <fct> <fct>             <fct>    \n1 $3    Plastic + sticker Chocolate\n```\n\n\n:::\n\n```{.r .cell-code}\npredict(model_mlogit, newdata = newdata_example_bad)\n```\n\n::: {.cell-output .cell-output-error}\n\n```\nError in predict.mlogit(model_mlogit, newdata = newdata_example_bad): the number of rows of the data.frame should be a multiple of the number of alternatives\n```\n\n\n:::\n:::\n\n\nInstead, because respondents were presented with two alternatives at a time, we need to feed `predict()` a data frame with two alternatives.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# The first two questions seen by two respondents\nnewdata_example <- stickers |> \n  filter(resp_id %in% c(4, 6), question == 1) |> \n  select(resp_id, price, packaging, flavor, alt)\nnewdata_example\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 4 x 5\n  resp_id price packaging         flavor      alt\n    <dbl> <fct> <fct>             <fct>     <dbl>\n1       4 $3    Plastic + sticker Chocolate     1\n2       4 $2    Plastic + paper   Nuts          2\n3       6 $4    Plastic + paper   Chocolate     1\n4       6 $2    Plastic + sticker Chocolate     2\n```\n\n\n:::\n\n```{.r .cell-code}\npredict(model_mlogit, newdata = newdata_example)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n          1         2\n1 0.8172620 0.1827380\n2 0.1042651 0.8957349\n```\n\n\n:::\n:::\n\n\n`predict()` returns a matrix of probabilities, with one row per respondent and one column for each alternative. In this case, respondent 4 had an 83% chance of choosing the \\$3 + sticker + chocolate alternative when presented alongside a $2 + paper + nuts alternative, while respondent 6 had an 11% chance of choosing the \\$4 + paper + chocolate alternative when presented alongside a \\$2 + sticker + chocolate alternative.\n\nTo make a balanced grid of feature attributes, we need to create a grid of all 12 unique combinations (3 prices, 2 packagings, 2 flavors = 3 × 2 × 2 = 12) paired with evert other unique combination of features. This requires a bit of data manipulation, including `cross_join()` which combines each row from the 12-row feature grid with each row from itself, resulting in 144 (12 × 12) rows. We then remove the 12 rows where the two alternatives are identical, resulting in a grid of 132 possible pairs of alternatives:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfeature_grid <- stickers |> \n  tidyr::expand(price, packaging, flavor)\n\n# Use cross_join to combine every row from the feature grid with itself\npaired_grid <- feature_grid |>\n  cross_join(\n    feature_grid |> \n      rename(\n        price_alt2 = price,\n        packaging_alt2 = packaging,\n        flavor_alt2 = flavor\n      )\n  ) |>\n  # Remove rows where both alternatives are identical\n  filter(\n    !(price == price_alt2 & \n      packaging == packaging_alt2 & \n      flavor == flavor_alt2)\n  ) |>\n  # Create unique choice_id identifier\n  mutate(choice_id = row_number())\n\npaired_grid\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 132 x 7\n   price packaging       flavor  price_alt2 packaging_alt2 flavor_alt2 choice_id\n   <fct> <fct>           <fct>   <fct>      <fct>          <fct>           <int>\n 1 $2    Plastic + paper Chocol~ $2         Plastic + pap~ Nuts                1\n 2 $2    Plastic + paper Chocol~ $2         Plastic + sti~ Chocolate           2\n 3 $2    Plastic + paper Chocol~ $2         Plastic + sti~ Nuts                3\n 4 $2    Plastic + paper Chocol~ $3         Plastic + pap~ Chocolate           4\n 5 $2    Plastic + paper Chocol~ $3         Plastic + pap~ Nuts                5\n 6 $2    Plastic + paper Chocol~ $3         Plastic + sti~ Chocolate           6\n 7 $2    Plastic + paper Chocol~ $3         Plastic + sti~ Nuts                7\n 8 $2    Plastic + paper Chocol~ $4         Plastic + pap~ Chocolate           8\n 9 $2    Plastic + paper Chocol~ $4         Plastic + pap~ Nuts                9\n10 $2    Plastic + paper Chocol~ $4         Plastic + sti~ Chocolate          10\n# i 122 more rows\n```\n\n\n:::\n:::\n\n\n{mlogit} requires long data for predictions, so we can stack the two alternatives on top of each other, resulting in a data frame with 264 rows (132 × 2):\n\n\n::: {.cell}\n\n```{.r .cell-code}\npaired_grid_long <- bind_rows(\n  # Alternative 1\n  paired_grid |>\n    select(choice_id, price, packaging, flavor) |>\n    mutate(alt = 1),\n  # Alternative 2\n  paired_grid |>\n    select(\n      choice_id,\n      price = price_alt2, \n      packaging = packaging_alt2,\n      flavor = flavor_alt2\n    ) |>\n    mutate(alt = 2)\n) |>\n  arrange(choice_id, alt)\npaired_grid_long\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 264 x 5\n   choice_id price packaging         flavor      alt\n       <int> <fct> <fct>             <fct>     <dbl>\n 1         1 $2    Plastic + paper   Chocolate     1\n 2         1 $2    Plastic + paper   Nuts          2\n 3         2 $2    Plastic + paper   Chocolate     1\n 4         2 $2    Plastic + sticker Chocolate     2\n 5         3 $2    Plastic + paper   Chocolate     1\n 6         3 $2    Plastic + sticker Nuts          2\n 7         4 $2    Plastic + paper   Chocolate     1\n 8         4 $3    Plastic + paper   Chocolate     2\n 9         5 $2    Plastic + paper   Chocolate     1\n10         5 $3    Plastic + paper   Nuts          2\n# i 254 more rows\n```\n\n\n:::\n:::\n\n\nFinally, we can generate predictions with this long data frame of all pairs of all combinations:\n\n\n::: {.cell}\n\n```{.r .cell-code}\npredictions <- predict(model_mlogit, newdata = paired_grid_long)\nhead(predictions)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n          1          2\n1 0.8448649 0.15513511\n2 0.3680205 0.63197951\n3 0.7602703 0.23972971\n4 0.6764910 0.32350895\n5 0.9192777 0.08072235\n6 0.5490849 0.45091508\n```\n\n\n:::\n:::\n\n\n`predict()` returns a matrix with 2 columns, but we're only really interested in one of them—we have a balanced grid of all possible pairs and only need to look at one half of each pair.\n\nWe can collapse this set of 132 predictions into the original balanced 12-row grid by calculating group specific means for price, packing, and flavor:\n\n\n::: {.cell}\n\n```{.r .cell-code}\npreds_grid_mlogit <- paired_grid_long |>\n  filter(alt == 1) |>\n  mutate(probability = predictions[,1]) |>\n  group_by(price, packaging, flavor) |>\n  summarize(estimate = mean(probability))\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\n`summarise()` has grouped output by 'price', 'packaging'. You can override\nusing the `.groups` argument.\n```\n\n\n:::\n\n```{.r .cell-code}\npreds_grid_mlogit\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 12 x 4\n# Groups:   price, packaging [6]\n   price packaging         flavor    estimate\n   <fct> <fct>             <fct>        <dbl>\n 1 $2    Plastic + paper   Chocolate    0.770\n 2 $2    Plastic + paper   Nuts         0.428\n 3 $2    Plastic + sticker Chocolate    0.853\n 4 $2    Plastic + sticker Nuts         0.544\n 5 $3    Plastic + paper   Chocolate    0.631\n 6 $3    Plastic + paper   Nuts         0.281\n 7 $3    Plastic + sticker Chocolate    0.735\n 8 $3    Plastic + sticker Nuts         0.387\n 9 $4    Plastic + paper   Chocolate    0.446\n10 $4    Plastic + paper   Nuts         0.141\n11 $4    Plastic + sticker Chocolate    0.562\n12 $4    Plastic + sticker Nuts         0.222\n```\n\n\n:::\n:::\n\n\nFinally, we calculate marginal means from this new grid by marginalizing or averaging across specific features of interest:\n\n\n::: {.cell}\n\n```{.r .cell-code}\npreds_grid_mlogit |> \n  group_by(packaging) |> \n  summarize(avg = mean(estimate))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 2 x 2\n  packaging           avg\n  <fct>             <dbl>\n1 Plastic + paper   0.449\n2 Plastic + sticker 0.551\n```\n\n\n:::\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\npaired_grid_long |>\n  filter(alt == 1) |>\n  mutate(probability = predictions[,1]) |>\n  group_by(packaging) |>\n  summarize(estimate = mean(probability))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 2 x 2\n  packaging         estimate\n  <fct>                <dbl>\n1 Plastic + paper      0.449\n2 Plastic + sticker    0.551\n```\n\n\n:::\n:::\n\n\n## What about standard errors and confidence intervals?\n\nWe can calculate marginal means, but so far we can only get averages and not any measures of uncertainty. With OLS, we were able to use {marginaleffects} to find both means and standard errors. Because of how {mlogit} deals with predictions, {marginaleffects} does not support it—if you try to feed an {mlogit}-based model into one of {marginaleffects}'s functions, you'll get this deprecation error:\n\n> Support for `mlogit` models was deprecated in version 0.23.0. The reason is that the data structure for these models is one observation-choice per row. Every other model-fitting package supported by `marginaleffects` treats rows as individual observations. The observation-choice structure made it harder to track indices and match individual predictions to rows in the original data. This added a lot of complexity to `marginaleffects`, and the results were not always reliable or safe.\n\nThe two most popular post-estimation packages—{marginaleffects} and {emmeans}—both struggle with multinomial models due to how they structure repeated data. `nnet::multinom()` is the only non-Bayesian multinomial package supported by both {marginaleffects} and {emmeans}, but as seen previously, it does not account for nested questions inside respondents.\n\nWe can measure the uncertainty of marginal means in a couple different ways:\n\n- The delta method, which requires manual matrix mulitplication and calculus to determine the gradient of the multinomial logistic function\n- Bootstrapping, which requires fitting hundreds of models on random subsets of the original data\n\nDetermining the gradient for the multinomial logistic distribution, especially with {mlogit}'s unique internal structuring of data, is surprisingly difficult. Again, neither {marginaleffects} nor {emmeans} can do it—{marginaleffects} tried for a while but gave up.\n\nBootstrapping, however, is a more flexible approach that requires no additional math or calculus, and it is farily straightforward with the {rsample} package. We can follow this general procedure:\n\n1. Randomly resample the original data with replacement some number of times\n2. Run the `mlogit()` model on each new sample\n3. Aggregate the results from each model, using the mean and the 2.5% and 97.5% percentiles for a confidence interval\n\n::: {.callout-tip}\n### Bootstrapping is flexible!\n\nThe example below uses {mlogit}, but the same approach will work for any of the multinomial logistic regression packages. As long as you can fit a model and generate predicted probabilities with it, you can repeat that process over and over on different versions of your data to simulate a confidence interval.\n:::\n\n### Randomly resample the original data\n\nFirst we'll use `bootstraps()` to randomly resample the original data 1,000 times. With data that lacks a nested structure, this is as straightforward as running `bootstraps(name_of_data, times = 1000)`. However, if we do that here, pairs of questions will be separated. Every respondent has 24 rows: 2 alternatives across 12 questions. We need to keep this respondent-level data together when resampling.\n\nOne way to ensure this happens is to group by respondent ID and then nest the remaining data into a list column:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nstickers |>\n  group_by(resp_id) |> \n  nest()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 295 x 2\n# Groups:   resp_id [295]\n   resp_id data              \n     <dbl> <list>            \n 1       4 <tibble [24 x 10]>\n 2       5 <tibble [24 x 10]>\n 3       6 <tibble [24 x 10]>\n 4       7 <tibble [24 x 10]>\n 5       8 <tibble [24 x 10]>\n 6       9 <tibble [24 x 10]>\n 7      10 <tibble [24 x 10]>\n 8      11 <tibble [24 x 10]>\n 9      12 <tibble [24 x 10]>\n10      13 <tibble [24 x 10]>\n# i 285 more rows\n```\n\n\n:::\n:::\n\n\nWhen {rsample} randomly resamples this data, it will keep the data associated with each respondent:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(rsample)\n\nset.seed(841630)  # From random.org\n\nbootstrapped_stickers <- stickers |>\n  group_by(resp_id) |> \n  nest() |> \n  ungroup() |> \n  bootstraps(\n    times = 1000\n  )\n```\n:::\n\n\nWe can confirm this if we look at one of the bootstrapped samples. Each respondent still has their associated data:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nbootstrapped_stickers$splits[[1]] |> analysis()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 295 x 2\n   resp_id data              \n     <dbl> <list>            \n 1      49 <tibble [24 x 10]>\n 2     170 <tibble [24 x 10]>\n 3      11 <tibble [24 x 10]>\n 4     102 <tibble [24 x 10]>\n 5     177 <tibble [24 x 10]>\n 6     254 <tibble [24 x 10]>\n 7     153 <tibble [24 x 10]>\n 8       5 <tibble [24 x 10]>\n 9     289 <tibble [24 x 10]>\n10     298 <tibble [24 x 10]>\n# i 285 more rows\n```\n\n\n:::\n:::\n\n\n### Run the model on each sample\n\nNext, we need to run `mlogit()` on each bootstrapped data frame. We already have the code for creating an indexed data frame, running a model, and generating predictions—we'll wrap all that up into a more general function:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfit_predict_mlogit <- function(.split, feature_grid, ...) {\n  .df <- as.data.frame(.split) |> \n    # Assign new unique respondent IDs (since some will be repeated through the\n    # bootstrapping process), and index the bootstrapped data frame so that it\n    # works with mlogit\n    mutate(resp_id = row_number()) |> \n    # Unnest the respondent-specific data\n    unnest(data) |> \n    group_by(resp_id, question) |> \n    mutate(choice_id = cur_group_id()) |> \n    ungroup() |> \n    as.data.frame() |> \n    dfidx(\n      idx = list(c(\"choice_id\", \"resp_id\"), \"alt\"),\n      choice = \"choice\",\n      shape = \"long\"\n    )\n\n  # Fit mlogit model\n  model <- mlogit(\n    choice ~ price + packaging + flavor,\n    data = .df\n  )\n\n  # Generate predicted probabilities on balanced feature grid\n  predictions <- predict(model, newdata = feature_grid)\n\n  # Aggregate predictions into feature-specific averages\n  feature_grid |> \n    filter(alt == 1) |> \n    mutate(estimate = predictions[, 1]) |> \n    group_by(price, packaging, flavor) |>\n    summarize(estimate = mean(estimate)) |> \n    ungroup()\n}\n```\n:::\n\n\nThen we'll feed each bootstrapped sample into our `fit_predict_mlogit()` function. This will take a while!\n\n\n::: {.cell}\n\n```{.r .cell-code}\nboot_results <- bootstrapped_stickers |>\n  mutate(boot_fits = map(splits, fit_predict_mlogit, feature_grid = paired_grid_long))\n```\n:::\n\n\n\n\nWe now have a column with the average predicted probabilities for each of the 12 combinations of conjoint features for each bootstrapped dataset. \n\n\n::: {.cell}\n\n```{.r .cell-code}\nboot_results\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# Bootstrap sampling \n# A tibble: 1,000 x 3\n   splits            id            boot_fits          \n   <list>            <chr>         <list>             \n 1 <split [295/106]> Bootstrap0001 <gropd_df [12 x 4]>\n 2 <split [295/113]> Bootstrap0002 <gropd_df [12 x 4]>\n 3 <split [295/114]> Bootstrap0003 <gropd_df [12 x 4]>\n 4 <split [295/110]> Bootstrap0004 <gropd_df [12 x 4]>\n 5 <split [295/113]> Bootstrap0005 <gropd_df [12 x 4]>\n 6 <split [295/117]> Bootstrap0006 <gropd_df [12 x 4]>\n 7 <split [295/113]> Bootstrap0007 <gropd_df [12 x 4]>\n 8 <split [295/106]> Bootstrap0008 <gropd_df [12 x 4]>\n 9 <split [295/113]> Bootstrap0009 <gropd_df [12 x 4]>\n10 <split [295/111]> Bootstrap0010 <gropd_df [12 x 4]>\n# i 990 more rows\n```\n\n\n:::\n:::\n\n\nHere's what one looks like:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nboot_results$boot_fits[[1]]\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 12 x 4\n# Groups:   price, packaging [6]\n   price packaging         flavor    estimate\n   <fct> <fct>             <fct>        <dbl>\n 1 $2    Plastic + paper   Chocolate    0.763\n 2 $2    Plastic + paper   Nuts         0.428\n 3 $2    Plastic + sticker Chocolate    0.852\n 4 $2    Plastic + sticker Nuts         0.553\n 5 $3    Plastic + paper   Chocolate    0.611\n 6 $3    Plastic + paper   Nuts         0.270\n 7 $3    Plastic + sticker Chocolate    0.726\n 8 $3    Plastic + sticker Nuts         0.385\n 9 $4    Plastic + paper   Chocolate    0.472\n10 $4    Plastic + paper   Nuts         0.163\n11 $4    Plastic + sticker Chocolate    0.597\n12 $4    Plastic + sticker Nuts         0.258\n```\n\n\n:::\n:::\n\n\n### Aggregate the results from each model\n\nAs before, we can calculate marginal means by calculating group averages for the different conjoint features in this balanced reference grid. Since we're working with 1,000 data frames instead of just 1, we'll need to use `map()` to group and summarize. The `estimate` column here shows the marginal mean for each packaging condition in each of the bootstrapped samples:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmms_packaging <- boot_results |> \n  mutate(mms = map(boot_fits, \\(.x) {\n    .x |> \n      group_by(packaging) |> \n      summarize(estimate = mean(estimate))\n  })) |> \n  unnest(mms)\nmms_packaging\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 2,000 x 5\n   splits            id            boot_fits           packaging        estimate\n   <list>            <chr>         <list>              <fct>               <dbl>\n 1 <split [295/106]> Bootstrap0001 <gropd_df [12 x 4]> Plastic + paper     0.451\n 2 <split [295/106]> Bootstrap0001 <gropd_df [12 x 4]> Plastic + stick~    0.562\n 3 <split [295/113]> Bootstrap0002 <gropd_df [12 x 4]> Plastic + paper     0.451\n 4 <split [295/113]> Bootstrap0002 <gropd_df [12 x 4]> Plastic + stick~    0.563\n 5 <split [295/114]> Bootstrap0003 <gropd_df [12 x 4]> Plastic + paper     0.446\n 6 <split [295/114]> Bootstrap0003 <gropd_df [12 x 4]> Plastic + stick~    0.552\n 7 <split [295/110]> Bootstrap0004 <gropd_df [12 x 4]> Plastic + paper     0.449\n 8 <split [295/110]> Bootstrap0004 <gropd_df [12 x 4]> Plastic + stick~    0.552\n 9 <split [295/113]> Bootstrap0005 <gropd_df [12 x 4]> Plastic + paper     0.442\n10 <split [295/113]> Bootstrap0005 <gropd_df [12 x 4]> Plastic + stick~    0.564\n# i 1,990 more rows\n```\n\n\n:::\n:::\n\n\nWe can visualize the distribution of these marginal means:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(mms_packaging, aes(x = estimate, fill = packaging)) +\n  geom_histogram(color = \"white\") +\n  guides(fill = \"none\") +\n  facet_wrap(vars(packaging), ncol = 1)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n```\n\n\n:::\n\n::: {.cell-output-display}\n![](mms-amces-multinomial_files/figure-pdf/unnamed-chunk-27-1.pdf){fig-pos='H'}\n:::\n:::\n\n\nAnd we can calculate confidence intervals based on percentiles. We can either use `quantile()` manually, or we can use this custom function to get a cleaner, more complete summary of the intervals:\n\n\n::: {.cell}\n\n```{.r .cell-code}\npercentile_ci <- function(x, alpha = 0.05) {\n  x <- na.omit(x)\n\n  lower <- quantile(x, probs = alpha / 2)\n  upper <- quantile(x, probs = 1 - alpha / 2)\n  estimate <- mean(x)\n\n  tibble(\n    .lower = lower,\n    .estimate = estimate,\n    .upper = upper,\n    .alpha = alpha,\n    .method = \"percentile\"\n  )\n}\n\nmms_packaging |> \n  group_by(packaging) |> \n  summarize(details = percentile_ci(estimate)) |> \n  unnest(details)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 2 x 6\n  packaging         .lower .estimate .upper .alpha .method   \n  <fct>              <dbl>     <dbl>  <dbl>  <dbl> <chr>     \n1 Plastic + paper    0.438     0.458  0.478   0.05 percentile\n2 Plastic + sticker  0.539     0.559  0.580   0.05 percentile\n```\n\n\n:::\n:::\n\n\nWe can calculate the marginal means individually for each conjoint feature, then combine them all into one large data frame for plotting and table-making.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmms_all <- boot_results |> \n  mutate(mms_price = map(boot_fits, \\(.x) {\n    .x |> \n      group_by(price) |> \n      summarize(estimate = mean(estimate))\n  })) |> \n  mutate(mms_packaging = map(boot_fits, \\(.x) {\n    .x |> \n      group_by(packaging) |> \n      summarize(estimate = mean(estimate))\n  })) |> \n  mutate(mms_flavor = map(boot_fits, \\(.x) {\n    .x |> \n      group_by(flavor) |> \n      summarize(estimate = mean(estimate))\n  }))\n\nmm_price_boot <- mms_all |> \n  unnest(mms_price) |> \n  group_by(attribute = price) |> \n  summarize(details = percentile_ci(estimate)) |> \n  unnest(details)\n\nmm_packaging_boot <- mms_all |> \n  unnest(mms_packaging) |> \n  group_by(attribute = packaging) |> \n  summarize(details = percentile_ci(estimate)) |> \n  unnest(details)\n\nmm_flavor_boot <- mms_all |> \n  unnest(mms_flavor) |> \n  group_by(attribute = flavor) |> \n  summarize(details = percentile_ci(estimate)) |> \n  unnest(details)\n\nmm_boot <- bind_rows(list(\n  \"Price\" = mm_price_boot,\n  \"Packaging\" = mm_packaging_boot,\n  \"Flavor\" = mm_flavor_boot\n), .id = \"feature\") |>\n  as_tibble()\n```\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(\n  mm_boot,\n  aes(x = .estimate, y = fct_rev(attribute), color = feature)\n) +\n  geom_vline(xintercept = 0.5) +\n  geom_pointrange(aes(xmin = .lower, xmax = .upper)) +\n  scale_x_continuous(labels = label_percent()) +\n  guides(color = \"none\") +\n  facet_col(vars(feature), scales = \"free_y\", space = \"free\")\n```\n\n::: {.cell-output-display}\n![](mms-amces-multinomial_files/figure-pdf/unnamed-chunk-30-1.pdf){fig-pos='H'}\n:::\n:::\n\n\n## Average marginal component effects (AMCEs)\n\nAverage marginal component effects (AMCEs) are differences in marginal means, where one attribute is used as a reference category. With OLS, we were able to calculate them automatically with `marginaleffects::avg_comparisons()`, but as seen above, {marginaleffects} can't work with {mlogit}. We have a balanced grid of predicted probabilities, though, which means we can find the differences in means ourselves with a little data wrangling.\n\n\n::: {.cell}\n\n```{.r .cell-code}\npreds_grid_mlogit |> \n  group_by(price) |> \n  summarize(mm = mean(estimate)) |> \n  mutate(amce = mm - mm[price == \"$2\"])\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 3 x 3\n  price    mm   amce\n  <fct> <dbl>  <dbl>\n1 $2    0.649  0    \n2 $3    0.508 -0.140\n3 $4    0.343 -0.306\n```\n\n\n:::\n\n```{.r .cell-code}\npreds_grid_mlogit |> \n  group_by(packaging) |> \n  summarize(mm = mean(estimate)) |> \n  mutate(amce = mm - mm[packaging == \"Plastic + paper\"])\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 2 x 3\n  packaging            mm  amce\n  <fct>             <dbl> <dbl>\n1 Plastic + paper   0.449 0    \n2 Plastic + sticker 0.551 0.101\n```\n\n\n:::\n\n```{.r .cell-code}\npreds_grid_mlogit |> \n  group_by(flavor) |> \n  summarize(mm = mean(estimate)) |> \n  mutate(amce = mm - mm[flavor == \"Nuts\"])\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 2 x 3\n  flavor       mm  amce\n  <fct>     <dbl> <dbl>\n1 Chocolate 0.666 0.332\n2 Nuts      0.334 0    \n```\n\n\n:::\n:::\n\n\nWe can go through the same process with the bootstrapped data as well to calculate the uncertainty for each AMCE:\n\n\n::: {.cell}\n\n```{.r .cell-code}\namces_all <- boot_results |> \n  mutate(mms_price = map(boot_fits, \\(.x) {\n    .x |> \n      group_by(price) |> \n      summarize(estimate = mean(estimate)) |> \n      mutate(amce = estimate - estimate[price == \"$2\"])\n  })) |> \n  mutate(mms_packaging = map(boot_fits, \\(.x) {\n    .x |> \n      group_by(packaging) |> \n      summarize(estimate = mean(estimate)) |> \n      mutate(amce = estimate - estimate[packaging == \"Plastic + paper\"])\n  })) |> \n  mutate(mms_flavor = map(boot_fits, \\(.x) {\n    .x |> \n      group_by(flavor) |> \n      summarize(estimate = mean(estimate)) |> \n      mutate(amce = estimate - estimate[flavor == \"Nuts\"])\n  }))\n\namces_price_boot <- amces_all |> \n  unnest(mms_price) |> \n  group_by(attribute = price) |> \n  summarize(details = percentile_ci(amce)) |> \n  unnest(details)\n\namces_packaging_boot <- amces_all |> \n  unnest(mms_packaging) |> \n  group_by(attribute = packaging) |> \n  summarize(details = percentile_ci(amce)) |> \n  unnest(details)\n\namces_flavor_boot <- amces_all |> \n  unnest(mms_flavor) |> \n  group_by(attribute = flavor) |> \n  summarize(details = percentile_ci(amce)) |> \n  unnest(details)\n\namces_boot <- bind_rows(list(\n  \"Price\" = amces_price_boot,\n  \"Packaging\" = amces_packaging_boot,\n  \"Flavor\" = amces_flavor_boot\n), .id = \"feature\") |>\n  as_tibble()\n```\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(\n  amces_boot,\n  aes(x = .estimate, y = fct_rev(attribute), color = feature)\n) +\n  geom_vline(xintercept = 0) +\n  geom_pointrange(aes(xmin = .lower, xmax = .upper)) +\n  scale_x_continuous(labels = label_pp) +\n  guides(color = \"none\") +\n  facet_col(vars(feature), scales = \"free_y\", space = \"free\")\n```\n\n::: {.cell-output-display}\n![](mms-amces-multinomial_files/figure-pdf/unnamed-chunk-33-1.pdf){fig-pos='H'}\n:::\n:::\n\n\n---\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel_logitr\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nlogitr(data = stickers_logitr, outcome = \"choice\", obsID = \"choice_id\",     pars = c(\"price\", \"packaging\", \"flavor\"))\n\nA Multinomial Logit model estimated in the Preference space\n\nExit Status: 3, Optimization stopped because ftol_rel or ftol_abs was reached.\n\nCoefficients:\n                   price$3                     price$4  \n                  -0.73770                    -1.60999  \npackagingPlastic + sticker                  flavorNuts  \n                   0.54072                    -1.69488  \n[1] -1670.548\n```\n\n\n:::\n\n```{.r .cell-code}\nalts <- stickers |>\n  tidyr::expand(price, packaging, flavor)\n\ncombn(nrow(alts), 2)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n     [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10] [,11] [,12] [,13] [,14]\n[1,]    1    1    1    1    1    1    1    1    1     1     1     2     2     2\n[2,]    2    3    4    5    6    7    8    9   10    11    12     3     4     5\n     [,15] [,16] [,17] [,18] [,19] [,20] [,21] [,22] [,23] [,24] [,25] [,26]\n[1,]     2     2     2     2     2     2     2     3     3     3     3     3\n[2,]     6     7     8     9    10    11    12     4     5     6     7     8\n     [,27] [,28] [,29] [,30] [,31] [,32] [,33] [,34] [,35] [,36] [,37] [,38]\n[1,]     3     3     3     3     4     4     4     4     4     4     4     4\n[2,]     9    10    11    12     5     6     7     8     9    10    11    12\n     [,39] [,40] [,41] [,42] [,43] [,44] [,45] [,46] [,47] [,48] [,49] [,50]\n[1,]     5     5     5     5     5     5     5     6     6     6     6     6\n[2,]     6     7     8     9    10    11    12     7     8     9    10    11\n     [,51] [,52] [,53] [,54] [,55] [,56] [,57] [,58] [,59] [,60] [,61] [,62]\n[1,]     6     7     7     7     7     7     8     8     8     8     9     9\n[2,]    12     8     9    10    11    12     9    10    11    12    10    11\n     [,63] [,64] [,65] [,66]\n[1,]     9    10    10    11\n[2,]    12    11    12    12\n```\n\n\n:::\n\n```{.r .cell-code}\nchoose(12, 2)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 66\n```\n\n\n:::\n\n```{.r .cell-code}\nn_alt <- 2\n\nchoice_sets_long <- alts |>\n  (\\(x) combn(seq_len(nrow(x)), m = n_alt, simplify = FALSE))() |>\n  map(\\(idx) {\n    alts[idx, ] |>\n      mutate(obsID = cur_group_id(), alt = seq_len(n_alt))\n  }) |>\n  list_rbind(names_to = \"obsID\") |>\n  mutate(obsID = as.integer(obsID)) |>\n  arrange(obsID, alt) |>\n  as_tibble()\nchoice_sets_long\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 132 x 5\n   price packaging         flavor    obsID   alt\n   <fct> <fct>             <fct>     <int> <int>\n 1 $2    Plastic + paper   Chocolate     1     1\n 2 $2    Plastic + paper   Nuts          1     2\n 3 $2    Plastic + paper   Chocolate     2     1\n 4 $2    Plastic + sticker Chocolate     2     2\n 5 $2    Plastic + paper   Chocolate     3     1\n 6 $2    Plastic + sticker Nuts          3     2\n 7 $2    Plastic + paper   Chocolate     4     1\n 8 $3    Plastic + paper   Chocolate     4     2\n 9 $2    Plastic + paper   Chocolate     5     1\n10 $3    Plastic + paper   Nuts          5     2\n# i 122 more rows\n```\n\n\n:::\n\n```{.r .cell-code}\npreds_logitr <- predict(\n  model_logitr,\n  newdata = choice_sets_long,\n  obsID = 'obsID',\n  returnData = TRUE\n)\n\npreds_logitr |>\n  group_by(packaging) |>\n  summarize(avg_pred = mean(predicted_prob))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 2 x 2\n  packaging         avg_pred\n  <fct>                <dbl>\n1 Plastic + paper      0.449\n2 Plastic + sticker    0.551\n```\n\n\n:::\n\n```{.r .cell-code}\nmms_logitr <- list(\n  \"Price\" = \"price\",\n  \"Packaging\" = \"packaging\",\n  \"Flavor\" = \"flavor\"\n) |>\n  map(\\(.x) {\n    preds_logitr |>\n      group_by(attribute = .data[[.x]]) |>\n      summarize(\n        estimate = mean(predicted_prob),\n        se = sd(predicted_prob) / sqrt(n()),\n        conf.low = estimate - 1.96 * se,\n        conf.high = estimate + 1.96 * se\n      )\n  }) |>\n  list_rbind(names_to = \"feature\")\n\nggplot(\n  mms_logitr,\n  aes(x = estimate, y = fct_rev(attribute), color = feature)\n) +\n  geom_vline(xintercept = 0.5) +\n  geom_pointrange(aes(xmin = conf.low, xmax = conf.high)) +\n  scale_x_continuous(labels = label_percent()) +\n  guides(color = \"none\") +\n  facet_col(vars(feature), scales = \"free_y\", space = \"free\")\n```\n\n::: {.cell-output-display}\n![](mms-amces-multinomial_files/figure-pdf/unnamed-chunk-34-1.pdf){fig-pos='H'}\n:::\n:::\n\n\n## Bootstrapping with {logitr}\n\nPhew, this is a messy attempt\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(rsample)\n\nset.seed(734498) # From random.org\n\nbootstrapped_stickers_logitr <- stickers |>\n  group_by(resp_id) |>\n  nest() |>\n  ungroup() |>\n  bootstraps(\n    times = 1000\n  )\n\nfit_logitr <- function(.split) {\n  library(dplyr)\n  library(tidyr)\n  library(logitr)\n\n  .df <- rsample::analysis(.split) |>\n    # Assign new unique respondent IDs (since some will be repeated through the\n    # bootstrapping process)\n    mutate(resp_id = row_number()) |>\n    # Unnest the respondent-specific data\n    unnest(data) |>\n    group_by(resp_id, question) |>\n    mutate(choice_id = cur_group_id()) |>\n    ungroup()\n\n  # Fit logitr model\n  model_logitr <- logitr(\n    data = .df,\n    outcome = \"choice\",\n    obsID = \"choice_id\",\n    pars = c(\"price\", \"packaging\", \"flavor\")\n  )\n\n  # # Generate predicted probabilities on balanced feature grid\n  # predictions <- predict(model, newdata = feature_grid)\n\n  # # Aggregate predictions into feature-specific averages\n  # feature_grid |>\n  #   filter(alt == 1) |>\n  #   mutate(estimate = predictions[, 1]) |>\n  #   group_by(price, packaging, flavor) |>\n  #   summarize(estimate = mean(estimate)) |>\n  #   ungroup()\n}\n\nmirai::daemons(10)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 10\n```\n\n\n:::\n\n```{.r .cell-code}\nboot_results_purrr <- bootstrapped_stickers_logitr |>\n  # slice(1:10) |>\n  mutate(\n    model = map(\n      splits,\n      in_parallel(\\(x) fit_logitr(x), fit_logitr = fit_logitr),\n      .progress = TRUE\n    )\n  )\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\n==>----------------------------    7% | ETA: 15s\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\n===>---------------------------   10% | ETA: 19s\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\n======>------------------------   20% | ETA: 21s\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\n=========>---------------------   29% | ETA: 20s\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\n===========>-------------------   35% | ETA: 20s\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\n==============>----------------   48% | ETA: 16s\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\n================>--------------   53% | ETA: 16s\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\n===================>-----------   64% | ETA: 11s\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\n======================>--------   74% | ETA:  9s\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\n========================>------   81% | ETA:  6s\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\n===========================>---   90% | ETA:  3s\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\n==============================>   98% | ETA:  0s\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\n==============================>  100% | ETA:  0s\n```\n\n\n:::\n\n```{.r .cell-code}\nmirai::daemons(0)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0\n```\n\n\n:::\n\n```{.r .cell-code}\ncalc_mms_logitr <- function(pr) {\n  list(\n    \"Price\" = \"price\",\n    \"Packaging\" = \"packaging\",\n    \"Flavor\" = \"flavor\"\n  ) |>\n    map(\\(.x) {\n      pr |>\n        group_by(attribute = .data[[.x]]) |>\n        summarize(\n          estimate = mean(estimate)\n        )\n    }) |>\n    list_rbind(names_to = \"feature\")\n}\n\n\nasdf <- boot_results_purrr |>\n  mutate(\n    preds = map(model, \\(.x) {\n      predict(\n        .x,\n        newdata = choice_sets_long,\n        obsID = 'obsID',\n        returnData = TRUE\n      ) |> \n        group_by(price, packaging, flavor) |>\n        summarize(estimate = mean(predicted_prob), .groups = \"drop\")\n    })\n  )\n\nasdfasdf <- asdf |> \n  mutate(mms = map(preds, \\(.preds) calc_mms_logitr(.preds)))\n\nasdfasdf |> \n  unnest(mms) |> \n  group_by(feature, attribute) |> \n  summarize(details = percentile_ci(estimate)) |> \n  unnest(details) |> \n  ggplot(aes(x = .estimate, y = fct_rev(attribute), color = feature)) +\n  geom_vline(xintercept = 0.5) +\n  geom_pointrange(aes(xmin = .lower, xmax = .upper)) +\n  scale_x_continuous(labels = label_percent()) +\n  guides(color = \"none\") +\n  facet_col(vars(feature), scales = \"free_y\", space = \"free\")\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\n`summarise()` has grouped output by 'feature'. You can override using the\n`.groups` argument.\n```\n\n\n:::\n\n::: {.cell-output-display}\n![](mms-amces-multinomial_files/figure-pdf/unnamed-chunk-35-1.pdf){fig-pos='H'}\n:::\n:::\n\n",
    "supporting": [
      "mms-amces-multinomial_files/figure-pdf"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {
      "knitr": [
        "{\"type\":\"list\",\"attributes\":{},\"value\":[]}"
      ]
    },
    "preserve": null,
    "postProcess": false
  }
}