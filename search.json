[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "The Conjoint Cookbook",
    "section": "",
    "text": "Running examples\nblah blah blah intro stuff here\nConjoint experiments or forced choice experiments (where respondents are shown randomly shuffled combinations of features and are asked to choose their most preferred option, and then they repeat that over and over again) have long been popular in marketing, and since 2014 they’ve become popular in polisci, public policy, and social science more broadly. But even though the actual approach is used the same way across disciplines, the estimands that disciplines are interested in are completely different.\nIn marketing, they care about consumer preferences and market shares, so they use conjoint data to build simulations of hypothetical product profiles, ultimately measuring market preferences.\nIn polisci, they care about casual effects—e.g. how much does the favorability of a political candidate change if they are a lawyer vs. not a lawyer. In this world, analysts don’t look at aggregate market preferences, but look at the exact causal levers and effects associated with different experimental conditions.\nThe two worlds know nothing about each other.\nMain point of book—bridging the two worlds of polisci-style causal effects (AMCEs and marginal means and AFCPs) with marketing/econ-style preference and utility descriptions + providing hands on code examples of the methods, from basic OLS from polisci to hierarchical Bayesian multinomial logit in marketing",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Overview</span>"
    ]
  },
  {
    "objectID": "index.html#running-examples",
    "href": "index.html#running-examples",
    "title": "The Conjoint Cookbook",
    "section": "",
    "text": "Product packaging\nStudy 5 from Sokolova, Krishna, and Döring (2023) (and data from ResearchBox)\n\n\n\nFeatures/Attributes\nLevels\n\n\n\n\nPrice\n$2, $3, $4\n\n\nPackaging\nPlastic + paper, Plastic + sticker\n\n\nFlavor\nNuts, Chocolate\n\n\n\n\n\n\n\n\n\nExample conjoint question/task\n\n\n\nIf these two granola bars were your only options, which would you choose?\n\n\n\n\n\n\n\n\nPrice\n$3\n$4\n\n\nPackaging\n\n\n\n\nFlavor\nChocolate chips\nNuts\n\n\nChoice\n\n\n\n\n\n\n\n\n\nPolitical candidates\nFrom Hainmueller, Hopkins, and Yamamoto (2014)\n\n\n\n\n\n\n\nFeatures/Attributes\nLevels\n\n\n\n\nMilitary service\nServed, Did not serve\n\n\nReligion\nNone, Jewish, Catholic, Mainline protestant, Evangelical protestant, Mormon\n\n\nCollege\nNo BA, Baptist college, Community college, State university, Small college, Ivy League university\n\n\nProfession\nBusiness owner, Lawyer, Doctor, High school teacher, Farmer, Car dealer\n\n\nGender\nMale, Female\n\n\nIncome\n$32,000; $54,000; $65,000; $92,000; $210,000; $5,100,000\n\n\nRace/Ethnicity\nWhite, Native American, Black, Hispanic, Caucasian, Asian American\n\n\nAge\n36, 45, 52, 60, 68, 75\n\n\n\n\n\n\n\n\n\nExample conjoint survey question\n\n\n\nIf you had to choose between them, which of these two candidates would you vote for?\n\n\n\n\n\n\n\n\n\nCandidate 1\nCandidate 2\n\n\n\n\nMilitary service\nDid not serve\nServed\n\n\nReligion\nNone\nMormon\n\n\nCollege\nState university\nIvy League university\n\n\nProfession\nLawyer\nBusiness owner\n\n\nGender\nFemale\nFemale\n\n\nIncome\n$54,000\n$92,000\n\n\nRace/Ethnicity\nWhite\nAsian American\n\n\nAge\n45\n68\n\n\nChoice\n\n\n\n\n\n\n\n\n\nMinivans\nFrom chapter 13 in Chapman and Feit (2019)\n\n\n\nFeatures/Attributes\nLevels\n\n\n\n\nPassengers\n6, 7, 8\n\n\nCargo area\n2 feet, 3 feet\n\n\nEngine\nGas, electric, hybrid\n\n\nPrice\n$30,000; $35,000; $40,000\n\n\n\n\n\n\n\n\n\nExample survey question\n\n\n\n\n\n\n\n\n\n\n\n\n\nOption 1\nOption 2\nOption 3\n\n\n\n\nPassengers\n7\n8\n6\n\n\nCargo area\n3 feet\n3 feet\n2 feet\n\n\nEngine\nElectric\nGas\nHybrid\n\n\nPrice\n$40,000\n$40,000\n$30,000\n\n\nChoice",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Overview</span>"
    ]
  },
  {
    "objectID": "index.html#references",
    "href": "index.html#references",
    "title": "The Conjoint Cookbook",
    "section": "References",
    "text": "References\n\n\n\n\nChapman, Chris, and Elea McDonnell Feit. 2019. R For Marketing Research and Analytics. 2nd ed. Use R! Cham, Switzerland: Springer Nature Switzerland. https://doi.org/10.1007/978-3-030-14316-9.\n\n\nHainmueller, Jens, Daniel J. Hopkins, and Teppei Yamamoto. 2014. “Causal Inference in Conjoint Analysis: Understanding Multidimensional Choices via Stated Preference Experiments.” Political Analysis 22 (1): 1–30. https://doi.org/10.1093/pan/mpt024.\n\n\nSokolova, Tatiana, Aradhna Krishna, and Tim Döring. 2023. “Paper Meets Plastic: The Perceived Environmental Friendliness of Product Packaging.” Journal of Consumer Research 50 (3): 468–91. https://doi.org/10.1093/jcr/ucad008.",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Overview</span>"
    ]
  },
  {
    "objectID": "mms-amces.html",
    "href": "mms-amces.html",
    "title": "2  Causal estimands",
    "section": "",
    "text": "2.1 Building the intuition for marginal means, differences in marginal means, and regression coefficients",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Causal estimands</span>"
    ]
  },
  {
    "objectID": "mms-amces.html#building-the-intuition-for-marginal-means-differences-in-marginal-means-and-regression-coefficients",
    "href": "mms-amces.html#building-the-intuition-for-marginal-means-differences-in-marginal-means-and-regression-coefficients",
    "title": "2  Causal estimands",
    "section": "",
    "text": "2.1.1 Marginal means\nAt its core, a “marginal mean” refers to the literal mean in the margins in a contingency table of model predictions based on a balanced grid of predictors.\nTo illustrate, we’ll make a model that predicts penguin weight based on species and sex and then make predictions for a balanced grid of covariates (i.e. male Adelie, female Adelie, male Chinstrap, female Chinstrap, male Gentoo, female Gentoo):\n\n\nCode\nlibrary(tidyverse)\nlibrary(marginaleffects)\n\n\n\n\nCode\npenguins &lt;- penguins |&gt; drop_na(sex)\n\nmodel &lt;- lm(body_mass ~ species + sex, data = penguins)\n\npreds &lt;- model |&gt; \n  predictions(datagrid(species = unique, sex = unique))\n\n\n\n\nCode\npreds |&gt;\n  select(estimate, species, sex) |&gt;\n  pivot_wider(names_from = sex, values_from = estimate) |&gt;\n  select(Species = species, Female = female, Male = male) |&gt;\n  tt(\n    notes = \"Predicted penguin weights (g) from model `lm(body_mass ~ species + sex)`\"\n  ) |&gt;\n  group_tt(\n    j = list(\"Sex\" = 2:3)\n  ) |&gt;\n  format_tt(\"notes\", markdown = TRUE) |&gt;\n  style_tt(j = 2:3, align = \"c\")\n\n\n\n\n    \n\n    \n    \n      \n        \n\n \nSex\n\n        \n              \n                Species\n                Female\n                Male\n              \n        \n        Predicted penguin weights (g) from model lm(body_mass ~ species + sex)\n        \n                \n                  Adelie\n                  3372\n                  4040\n                \n                \n                  Gentoo\n                  4750\n                  5418\n                \n                \n                  Chinstrap\n                  3399\n                  4067\n                \n        \n      \n    \n\n\n\nWe can add a column and row in the margins to show the species-specific and sex-specific average predicted weights. For the sex-specific averages, all the between-species variation is “marginalized out” or accounted for. For the species-specific averages, all the between-sex variation is similarly marginalized out:\n\n\nCode\nmean_sex &lt;- preds |&gt;\n  group_by(sex) |&gt;\n  mutate(avg_estimate = mean(estimate)) |&gt;\n  select(species, sex, estimate, avg_estimate) |&gt;\n  pivot_wider(names_from = species, values_from = estimate) |&gt;\n  mutate(\n    avg_explanation = glue::glue(\n      \"{round(avg_estimate, 0)}&lt;br&gt;\",\n      \"&lt;span style='font-size:70%'&gt;\",\n      \"({round(Adelie, 0)} + {round(Chinstrap, 0)} + {round(Gentoo, 0)}) / 3\",\n      \"&lt;/span&gt;\"\n    )\n  ) |&gt;\n  select(sex, avg_explanation) |&gt;\n  mutate(species = \"Marginal mean\") |&gt;\n  pivot_wider(names_from = sex, values_from = avg_explanation)\n\nmean_species &lt;- preds |&gt;\n  group_by(species) |&gt;\n  summarize(avg_estimate = mean(estimate))\n\nall_combined &lt;- preds |&gt;\n  ungroup() |&gt;\n  select(estimate, species, sex) |&gt;\n  pivot_wider(names_from = sex, values_from = estimate) |&gt;\n  left_join(mean_species, by = join_by(species)) |&gt;\n  arrange(species) |&gt;\n  mutate(\n    avg_explanation = glue::glue(\n      \"{round(avg_estimate, 0)}&lt;br&gt;\",\n      \"&lt;span style='font-size:70%'&gt;\",\n      \"({round(female, 0)} + {round(male, 0)}) / 2\",\n      \"&lt;/span&gt;\"\n    )\n  ) |&gt;\n  mutate(across(c(female, male), ~ as.character(round(., 0)))) |&gt;\n  select(-avg_estimate) |&gt;\n  bind_rows(mean_sex)\n\nall_combined |&gt;\n  select(\n    Species = species,\n    Female = female,\n    Male = male,\n    `Marginal mean` = avg_explanation\n  ) |&gt;\n  tt(\n    notes = \"Predicted penguin weights (g) from model `lm(body_mass ~ species + sex)`\"\n  ) |&gt;\n  group_tt(\n    j = list(\"Sex\" = 2:3)\n  ) |&gt;\n  format_tt(replace = \"\") |&gt;\n  format_tt(\"notes\", markdown = TRUE) |&gt;\n  style_tt(j = 2:4, align = \"c\") |&gt;\n  style_tt(i = 4, line = \"t\") |&gt;\n  theme_html(i = 4, css = \"border-top-style: dashed;\") |&gt;\n  style_tt(i = 1:4, j = 3, line = \"r\") |&gt;\n  theme_html(j = 3, css = \"border-right-style: dashed;\")\n\n\n\n\n    \n\n    \n    \n      \n        \n\n \nSex\n \n\n        \n              \n                Species\n                Female\n                Male\n                Marginal mean\n              \n        \n        Predicted penguin weights (g) from model lm(body_mass ~ species + sex)\n        \n                \n                  Adelie\n                  3372\n                  4040\n                  3706(3372 + 4040) / 2\n                \n                \n                  Chinstrap\n                  3399\n                  4067\n                  3733(3399 + 4067) / 2\n                \n                \n                  Gentoo\n                  4750\n                  5418\n                  5084(4750 + 5418) / 2\n                \n                \n                  Marginal mean\n                  3841(3372 + 3399 + 4750) / 3\n                  4508(4040 + 4067 + 5418) / 3\n                   \n                \n        \n      \n    \n\n\n\n\n\n2.1.2 Differences in marginal means\nBecause regression is just fancy averages, the differences in marginal means here are actually identical to the coefficients in regression model. For instance, here are the results from the model:\n\n\nCode\nmodel |&gt; broom::tidy()\n\n\n# A tibble: 4 × 5\n  term             estimate std.error statistic   p.value\n  &lt;chr&gt;               &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n1 (Intercept)        3372.       31.4   107.    4.34e-258\n2 speciesChinstrap     26.9      46.5     0.579 5.63e-  1\n3 speciesGentoo      1378.       39.1    35.2   1.05e-113\n4 sexmale             668.       34.7    19.2   8.73e- 56\n\n\nThe sexmale coefficient of 667.56 represents the effect of moving from male to female when species is held constant.\nThe marginal means table shows the same thing. Marginalizing over species (i.e. holding species constant), the average predicted weight for female penguins is 3840.6 and for male penguins is 4508.2. The difference in those marginal means is 667.56—identical to the sexmale coefficient!\n\n\nCode\namce_species &lt;- preds |&gt;\n  group_by(species) |&gt;\n  summarize(avg_estimate = mean(estimate)) |&gt;\n  mutate(amce = avg_estimate - avg_estimate[1]) |&gt;\n  mutate(\n    amce_nice = glue::glue(\n      \"{round(amce, 0)}&lt;br&gt;\",\n      \"&lt;span style='font-size:70%'&gt;\",\n      \"{species} − {species[1]}, or&lt;br&gt;\",\n      \"{round(avg_estimate, 0)} − {round(avg_estimate[1], 0)}\",\n      \"&lt;/span&gt;\"\n    )\n  ) |&gt;\n  select(species, amce_nice)\n\namce_sex &lt;- preds |&gt;\n  group_by(sex) |&gt;\n  summarize(avg_estimate = mean(estimate)) |&gt;\n  mutate(amce = avg_estimate - avg_estimate[1]) |&gt;\n  mutate(\n    amce_nice = glue::glue(\n      \"{round(amce, 0)}&lt;br&gt;\",\n      \"&lt;span style='font-size:70%'&gt;\",\n      \"{sex} − {sex[1]}, or&lt;br&gt;\",\n      \"{round(avg_estimate, 0)} − {round(avg_estimate[1], 0)}\",\n      \"&lt;/span&gt;\"\n    )\n  ) |&gt;\n  select(sex, amce_nice) |&gt;\n  mutate(species = \"∆ in marginal mean\") |&gt;\n  pivot_wider(names_from = sex, values_from = amce_nice)\n\n\n\n\nCode\nall_combined |&gt;\n  left_join(amce_species, by = join_by(species)) |&gt;\n  bind_rows(amce_sex) |&gt;\n  select(\n    Species = species,\n    Female = female,\n    Male = male,\n    `Marginal mean` = avg_explanation,\n    `∆ in marginal mean` = amce_nice\n  ) |&gt;\n  tt(\n    notes = \"Predicted penguin weights (g) from model `lm(body_mass ~ species + sex)`\"\n  ) |&gt;\n  group_tt(\n    j = list(\"Sex\" = 2:3)\n  ) |&gt;\n  format_tt(replace = \"\") |&gt;\n  format_tt(\"notes\", markdown = TRUE) |&gt;\n  style_tt(j = 2:5, align = \"c\") |&gt;\n  style_tt(i = 4, line = \"t\") |&gt;\n  theme_html(i = 4, css = \"border-top-style: dashed;\") |&gt;\n  style_tt(i = 1:5, j = 3, line = \"r\") |&gt;\n  theme_html(j = 3, css = \"border-right-style: dashed;\")\n\n\n\n\n    \n\n    \n    \n      \n        \n\n \nSex\n \n \n\n        \n              \n                Species\n                Female\n                Male\n                Marginal mean\n                ∆ in marginal mean\n              \n        \n        Predicted penguin weights (g) from model lm(body_mass ~ species + sex)\n        \n                \n                  Adelie\n                  3372\n                  4040\n                  3706(3372 + 4040) / 2\n                  0Adelie − Adelie, or3706 − 3706\n                \n                \n                  Chinstrap\n                  3399\n                  4067\n                  3733(3399 + 4067) / 2\n                  27Chinstrap − Adelie, or3733 − 3706\n                \n                \n                  Gentoo\n                  4750\n                  5418\n                  5084(4750 + 5418) / 2\n                  1378Gentoo − Adelie, or5084 − 3706\n                \n                \n                  Marginal mean\n                  3841(3372 + 3399 + 4750) / 3\n                  4508(4040 + 4067 + 5418) / 3\n                   \n                   \n                \n                \n                  ∆ in marginal mean\n                  0female − female, or3841 − 3841\n                  668male − female, or4508 − 3841\n                   \n                   \n                \n        \n      \n    \n\n\n\nDifferences in marginal means are equivalent to marginal effects or regression coefficients.\n\n\n2.1.3 Faster way with {marginaleffects}\nThese manual calculations are a pain though. We can use {marginaleffects} to do it more directly:\n\n\nCode\navg_predictions(model, \n  newdata = datagrid(species = unique, sex = unique),\n  by = c(\"sex\", \"species\")\n)\n\n\n\n    sex   species Estimate Std. Error     z Pr(&gt;|z|)   S 2.5 % 97.5 %\n female Adelie        3372       31.4 107.3   &lt;0.001 Inf  3311   3434\n female Chinstrap     3399       42.1  80.7   &lt;0.001 Inf  3317   3482\n female Gentoo        4750       34.0 139.5   &lt;0.001 Inf  4684   4817\n male   Adelie        4040       31.4 128.5   &lt;0.001 Inf  3978   4102\n male   Chinstrap     4067       42.1  96.5   &lt;0.001 Inf  3984   4149\n male   Gentoo        5418       33.6 161.3   &lt;0.001 Inf  5352   5484\n\nType: response\n\n\nCode\navg_comparisons(model, \n  newdata = datagrid(species = unique, sex = unique)\n)\n\n\n\n    Term           Contrast Estimate Std. Error      z Pr(&gt;|z|)     S  2.5 % 97.5 %\n sex     male - female         667.6       34.7 19.236   &lt;0.001 271.5  599.5    736\n species Chinstrap - Adelie     26.9       46.5  0.579    0.562   0.8  -64.2    118\n species Gentoo - Adelie      1377.9       39.1 35.236   &lt;0.001 901.1 1301.2   1455\n\nType: response",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Causal estimands</span>"
    ]
  },
  {
    "objectID": "mms-amces.html#conjoint-designs-marginal-means-and-amces",
    "href": "mms-amces.html#conjoint-designs-marginal-means-and-amces",
    "title": "2  Causal estimands",
    "section": "2.2 Conjoint designs, marginal means, and AMCEs",
    "text": "2.2 Conjoint designs, marginal means, and AMCEs\nMarginal means in conjoints represent the probability-scale fitted values from the model, calculated across a balanced reference grid of all possible combinations of feature levels. These predictions are then marginalized or averaged across features of interest and result in causal estimands like the AMCE.\nIn the case of the sticker experiment, there are 12 possible combinations of price, packaging, and flavor:\n\n\nCode\nstickers &lt;- readRDS(\"data/processed_data/study_5_sticker.rds\")\n\nmodel_ols &lt;- lm(\n  choice ~ price + packaging + flavor,\n  data = stickers\n)\n\n\n\n\nCode\nfeature_grid &lt;- stickers |&gt; \n  tidyr::expand(price, packaging, flavor)\ntt(feature_grid)\n\n\n\n\n    \n\n    \n    \n      \n        \n        \n              \n                price\n                packaging\n                flavor\n              \n        \n        \n        \n                \n                  $2\n                  Plastic + paper\n                  Chocolate\n                \n                \n                  $2\n                  Plastic + paper\n                  Nuts\n                \n                \n                  $2\n                  Plastic + sticker\n                  Chocolate\n                \n                \n                  $2\n                  Plastic + sticker\n                  Nuts\n                \n                \n                  $3\n                  Plastic + paper\n                  Chocolate\n                \n                \n                  $3\n                  Plastic + paper\n                  Nuts\n                \n                \n                  $3\n                  Plastic + sticker\n                  Chocolate\n                \n                \n                  $3\n                  Plastic + sticker\n                  Nuts\n                \n                \n                  $4\n                  Plastic + paper\n                  Chocolate\n                \n                \n                  $4\n                  Plastic + paper\n                  Nuts\n                \n                \n                  $4\n                  Plastic + sticker\n                  Chocolate\n                \n                \n                  $4\n                  Plastic + sticker\n                  Nuts\n                \n        \n      \n    \n\n\n\nWe can feed each row of this balanced grid into the model to generate 12 predicted values:\n\n\nCode\npredictions(model_ols, newdata = feature_grid)\n\n\n\n Estimate Std. Error     z Pr(&gt;|z|)     S  2.5 % 97.5 %\n   0.8003     0.0115 69.39   &lt;0.001   Inf 0.7777 0.8229\n   0.3933     0.0115 34.10   &lt;0.001 844.2 0.3707 0.4159\n   0.9153     0.0115 79.39   &lt;0.001   Inf 0.8927 0.9379\n   0.5082     0.0115 44.07   &lt;0.001   Inf 0.4856 0.5308\n   0.6565     0.0115 56.91   &lt;0.001   Inf 0.6339 0.6791\n   0.2495     0.0115 21.63   &lt;0.001 342.4 0.2269 0.2721\n   0.7715     0.0115 66.88   &lt;0.001   Inf 0.7489 0.7941\n   0.3645     0.0115 31.59   &lt;0.001 725.3 0.3419 0.3871\n   0.4815     0.0115 41.73   &lt;0.001   Inf 0.4589 0.5042\n   0.0745     0.0115  6.46   &lt;0.001  33.2 0.0519 0.0971\n   0.5965     0.0115 51.71   &lt;0.001   Inf 0.5739 0.6191\n   0.1895     0.0115 16.43   &lt;0.001 199.2 0.1669 0.2121\n\nType: response\n\n\nFinally, we can marginalize or average these predicted values across features of interest. For instance, to find the marginal means for the two packaging conditions, we can calculate the group averages for the two types of packaging:\n\n\nCode\npredictions(model_ols, newdata = feature_grid) |&gt; \n  group_by(packaging) |&gt; \n  summarize(avg = mean(estimate))\n\n\n# A tibble: 2 × 2\n  packaging           avg\n  &lt;fct&gt;             &lt;dbl&gt;\n1 Plastic + paper   0.443\n2 Plastic + sticker 0.558\n\n\nManually creating a balanced reference grid and using group_by() and summarize() is useful for understanding the intuition behind finding estimated marginal means, but in practice it is better to use avg_predictions() from {marginaleffects}, which (1) creates the balanced grid automatically, (2) provides standard errors and other estimates of uncertainty, and (3) can adjust the standard errors to account for repeated respondents:\n\n\nCode\navg_predictions(\n  model_ols,\n  newdata = \"balanced\",\n  by = \"packaging\",\n  vcov = ~resp_id\n)\n\n\n\n         packaging Estimate Std. Error    z Pr(&gt;|z|)   S 2.5 % 97.5 %\n Plastic + paper      0.443    0.00868 51.0   &lt;0.001 Inf 0.426  0.460\n Plastic + sticker    0.558    0.00881 63.3   &lt;0.001 Inf 0.540  0.575\n\nType: response",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Causal estimands</span>"
    ]
  },
  {
    "objectID": "utils-preds.html",
    "href": "utils-preds.html",
    "title": "3  Descriptive estimands",
    "section": "",
    "text": "3.1 Population-level stuff",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Descriptive estimands</span>"
    ]
  },
  {
    "objectID": "utils-preds.html#population-level-stuff",
    "href": "utils-preds.html#population-level-stuff",
    "title": "3  Descriptive estimands",
    "section": "",
    "text": "3.1.1 Model βs\n\n\n\n\n    \n\n    \n    \n      \n        \n        \n              \n                Parameter\n                Coefficient\n                SE\n                CI_low\n                CI_high\n                z\n                p\n              \n        \n        \n        \n                \n                  price$3\n                  -1.82\n                  0.15\n                  -2.11\n                  -1.53\n                  -12.33\n                  &lt;0.001\n                \n                \n                  price$4\n                  -3.98\n                  0.22\n                  -4.42\n                  -3.54\n                  -17.79\n                  &lt;0.001\n                \n                \n                  packagingPlastic + sticker\n                  1.23\n                  0.12\n                  0.99\n                  1.47\n                  10.00\n                  &lt;0.001\n                \n                \n                  flavorNuts\n                  -3.93\n                  0.23\n                  -4.38\n                  -3.48\n                  -17.06\n                  &lt;0.001",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Descriptive estimands</span>"
    ]
  },
  {
    "objectID": "utils-preds.html#individual-level-stuff-part-worth-utilities-and-ratios",
    "href": "utils-preds.html#individual-level-stuff-part-worth-utilities-and-ratios",
    "title": "3  Descriptive estimands",
    "section": "3.2 Individual-level stuff: Part-worth utilities and ratios",
    "text": "3.2 Individual-level stuff: Part-worth utilities and ratios\n\n\n\n\n    \n\n    \n    \n      \n        \n\n \nPrice\nPackaging\nFlavor\n\n        \n              \n                ID\n                $2\n                $3\n                $4\n                Paper\n                Sticker\n                Chocolate\n                Nuts\n              \n        \n        \n        \n                \n                  4\n                  0\n                  -2.22\n                  -3.98\n                  0\n                  -4.24\n                  0\n                  -9.76\n                \n                \n                  5\n                  0\n                  -2.55\n                  -5.99\n                  0\n                  -0.19\n                  0\n                  -0.14\n                \n                \n                  6\n                  0\n                  -1.31\n                  -2.87\n                  0\n                  2.67\n                  0\n                  -8.60\n                \n                \n                  7\n                  0\n                  -1.61\n                  -3.34\n                  0\n                  2.63\n                  0\n                  -6.78\n                \n                \n                  8\n                  0\n                  -1.57\n                  -3.71\n                  0\n                  3.49\n                  0\n                  -4.41\n                \n        \n      \n    \n\n\n\nFor respondent 4, the difference in preference when moving from $2 to $4 is roughly the same as the preference for a sticker\nWe can also calculate the relative importance of each attribute for each individual by determining how much each attribute contributes to the overall utility of the choice. We first calculate the range of each\n\n\n`summarise()` has grouped output by 'id'. You can override using the `.groups` argument.\n\n\n\n\n    \n\n    \n    \n      \n        \n        \n              \n                Feature\n                max(βi) − min(βi)\n                Range\n                Importance\n              \n        \n        \n        \n                \n                  Price\n                  0 − -3.98\n                  3.98\n                  22.1%\n                \n                \n                  Packaging\n                  0 − -4.24\n                  4.24\n                  23.6%\n                \n                \n                  Flavor\n                  0 − -9.76\n                  9.76\n                  54.3%\n                \n                \n                  Total\n                  \n                  17.99\n                  100.0%\n                \n        \n      \n    \n\n\n\n\n\n\n\n    \n\n    \n    \n      \n        \n\n \nRange\nImportance\n\n        \n              \n                ID\n                Price\n                Packaging\n                Flavor\n                Price \n                Packaging \n                Flavor \n              \n        \n        \n        \n                \n                  4\n                  3.98\n                  4.24\n                  9.76\n                  22.1%\n                  23.6%\n                  54.3%\n                \n                \n                  5\n                  5.99\n                  0.19\n                  0.14\n                  94.7%\n                  3.1%\n                  2.2%\n                \n                \n                  6\n                  2.87\n                  2.67\n                  8.60\n                  20.3%\n                  18.9%\n                  60.8%\n                \n                \n                  7\n                  3.34\n                  2.63\n                  6.78\n                  26.2%\n                  20.6%\n                  53.2%\n                \n                \n                  8\n                  3.71\n                  3.49\n                  4.41\n                  32.0%\n                  30.0%\n                  38.0%\n                \n        \n      \n    \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n`summarise()` has grouped output by 'feature'. You can override using the `.groups` argument.\n\n\n\n\n\n\n\n\n\n\ncor.mlogit(model_mlogit_hierarchical) |&gt; \n  as_tibble(rownames = \"coefficient\") |&gt; \n  tt() |&gt; \n  format_tt(j = 2:5, fn = scales::label_number(accuracy = 0.001))\n\n\n\n    \n\n    \n    \n      \n        \n        \n              \n                coefficient\n                price$3\n                price$4\n                packagingPlastic + sticker\n                flavorNuts\n              \n        \n        \n        \n                \n                  price$3\n                  1.000\n                  0.899\n                  0.727\n                  0.069\n                \n                \n                  price$4\n                  0.899\n                  1.000\n                  0.529\n                  0.132\n                \n                \n                  packagingPlastic + sticker\n                  0.727\n                  0.529\n                  1.000\n                  0.240\n                \n                \n                  flavorNuts\n                  0.069\n                  0.132\n                  0.240\n                  1.000",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Descriptive estimands</span>"
    ]
  },
  {
    "objectID": "utils-preds.html#predictions-and-transformations",
    "href": "utils-preds.html#predictions-and-transformations",
    "title": "3  Descriptive estimands",
    "section": "3.3 Predictions and transformations",
    "text": "3.3 Predictions and transformations\n\n3.3.1 Willingness-to-pay?\n\n\n3.3.2 Simulated choice shares\nFunctions from @Feit:2019\n\npredict_mnl &lt;- function(model, data) {\n  # Function for predicting shares from a multinomial logit model \n  # model: mlogit object returned by mlogit()\n  # data: a data frame containing the set of designs for which you want to \n  #       predict shares. Same format at the data used to estimate model. \n  data.model &lt;- model.matrix(update(model$formula, 0 ~ .), data = data)[ , -1]\n  utility &lt;- data.model %*% model$coef\n  share &lt;- exp(utility) / sum(exp(utility))\n  cbind(share, data)\n}\n\npredict_hier_mnl &lt;- function(model, data, nresp =1000) { \n  # Function to predict shares of a hierarchical multinomial logit model\n  # model: mlogit object returned by mlogit()\n  # data: a data frame containing the set of designs for which you want to\n  # predict shares. Same format at the data used to estimate model.\n  # Note that this code assumes all model parameters are random\n  data.model &lt;- model.matrix(update(model$formula , 0 ~ .), data = data)[ , -1]\n  coef.Sigma &lt;- cov.mlogit(model)\n  coef.mu &lt;- model$coef[1:dim(coef.Sigma)[1]]\n  draws &lt;- MASS::mvrnorm(n = nresp, coef.mu, coef.Sigma)\n  shares &lt;- matrix(NA, nrow = nresp, ncol = nrow(data))\n\n  for (i in 1:nresp) {\n    utility &lt;- data.model%*%draws[i,]\n    share &lt;- exp(utility)/sum(exp(utility))\n    shares[i,] &lt;- share \n  }\n\n  cbind(colMeans(shares), data)\n}\n\nexample_product_mix &lt;- tribble(\n  ~price, ~packaging, ~flavor,\n  \"$2\", \"Plastic + sticker\", \"Chocolate\",\n  \"$3\", \"Plastic + sticker\", \"Chocolate\",\n  \"$4\", \"Plastic + sticker\", \"Chocolate\",\n  \"$2\", \"Plastic + paper\", \"Nuts\",\n  \"$3\", \"Plastic + paper\", \"Nuts\",\n  \"$4\", \"Plastic + paper\", \"Nuts\"\n) |&gt; \n  mutate(across(everything(), factor))\n\npredict_hier_mnl(model_mlogit_hierarchical, example_product_mix)\n\n  colMeans(shares) price         packaging    flavor\n1         0.645363    $2 Plastic + sticker Chocolate\n2         0.121951    $3 Plastic + sticker Chocolate\n3         0.045006    $4 Plastic + sticker Chocolate\n4         0.157910    $2   Plastic + paper      Nuts\n5         0.021582    $3   Plastic + paper      Nuts\n6         0.008187    $4   Plastic + paper      Nuts",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Descriptive estimands</span>"
    ]
  },
  {
    "objectID": "utils-preds.html#market-simulations",
    "href": "utils-preds.html#market-simulations",
    "title": "3  Descriptive estimands",
    "section": "3.4 Market simulations?",
    "text": "3.4 Market simulations?",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Descriptive estimands</span>"
    ]
  },
  {
    "objectID": "process-reshape-data.html",
    "href": "process-reshape-data.html",
    "title": "6  Clean and join data",
    "section": "",
    "text": "6.1 Individual-level responses\nPopular conjoint survey platforms like Qualtrics and Sawtooth typically provide results in two separate data files: (1) individual participant-level responses and (2) alternative-level combinations of features that each respondent saw. To analyze the results of a conjoint experiment using regression, the two datasets need to be joined.\nWith individual-level responses, each row represents a survey respondent and each column represents a survey question presented to the respondent. If the survey included general questions about the respondent’s demographics, education, income, or any other question, those responses appear here. This data also includes a column for each conjoint task presented to the respondent indicating which of the alternatives was selected.\nFor instance, in this example data, researchers collected data on respondent gender and age before asking a series of 12 conjoint tasks. The data thus has columns for gender and age (respondent-level characteristics), and CBC_Random{N} (the choices selected during each conjoint task):\nresponses &lt;- readRDS(here::here(\"data\", \"processed_data\", \"responses_illustration.rds\"))\nresponses\n\n# A tibble: 295 × 15\n   resp_id gender   age CBC_Random1 CBC_Random2 CBC_Random3 CBC_Random4\n     &lt;dbl&gt; &lt;chr&gt;  &lt;dbl&gt;       &lt;dbl&gt;       &lt;dbl&gt;       &lt;dbl&gt;       &lt;dbl&gt;\n 1       4 Female    19           1           2           1           1\n 2       5 Female    19           2           1           2           2\n 3       6 Male      20           2           1           1           2\n 4       7 Female    20           2           2           1           2\n 5       8 Female    20           2           2           2           2\n 6       9 Male      20           1           2           1           1\n 7      10 Female    19           2           2           2           1\n 8      11 Male      22           2           1           1           1\n 9      12 Male      20           1           1           2           1\n10      13 Female    19           2           2           2           2\n# ℹ 285 more rows\n# ℹ 8 more variables: CBC_Random5 &lt;dbl&gt;, CBC_Random6 &lt;dbl&gt;, CBC_Random7 &lt;dbl&gt;,\n#   CBC_Random8 &lt;dbl&gt;, CBC_Random9 &lt;dbl&gt;, CBC_Random10 &lt;dbl&gt;,\n#   CBC_Random11 &lt;dbl&gt;, CBC_Random12 &lt;dbl&gt;",
    "crumbs": [
      "Fielding",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Clean and join data</span>"
    ]
  },
  {
    "objectID": "process-reshape-data.html#possible-alternatives",
    "href": "process-reshape-data.html#possible-alternatives",
    "title": "6  Clean and join data",
    "section": "6.2 Possible alternatives",
    "text": "6.2 Possible alternatives\nIn this example, the CBC_Random{N} columns show which of the two alternatives were selected in each task. However, they do not provide the complete context of the experimental task, like which combinations of levels were displayed with each feature. This information is crucial for conjoint analysis, since we are trying to statistically detect the salience of specific levels and features in relation to others.\nAlternative-level data provides this context. In this data, each row represents one of the choices presented to a respondent, with a column for each feature and the alternative shown and a column linking the row to the respondent ID (version in this example; the exact name of the column varies across survey platforms). Some survey platforms include a column indicating which choice was selected; this example data does not and instead includes it in responses.\n\nalternatives &lt;- readRDS(here::here(\"data\", \"processed_data\", \"alternatives_illustration.rds\"))\nalternatives\n\n# A tibble: 7,080 × 6\n   version question   alt price packaging         flavor   \n     &lt;dbl&gt;    &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt; &lt;fct&gt;             &lt;fct&gt;    \n 1       4        1     1 $3    Plastic + sticker Chocolate\n 2       4        1     2 $2    Plastic + paper   Nuts     \n 3       4        2     1 $3    Plastic + sticker Nuts     \n 4       4        2     2 $2    Plastic + paper   Chocolate\n 5       4        3     1 $4    Plastic + paper   Chocolate\n 6       4        3     2 $2    Plastic + sticker Chocolate\n 7       4        4     1 $2    Plastic + sticker Chocolate\n 8       4        4     2 $4    Plastic + paper   Nuts     \n 9       4        5     1 $4    Plastic + sticker Chocolate\n10       4        5     2 $2    Plastic + paper   Nuts     \n# ℹ 7,070 more rows\n\n\nSince there are 2 alternatives presented in each conjoint question, and there are 12 questions or tasks, each respondent has 24 associated rows. With 295 respondents, the alternatives data thus contains 295 × 12 × 2, or 7,080 rows:\n\nalternatives |&gt; \n  summarize(\n    versions = n_distinct(version),\n    questions = n_distinct(question),\n    alts = n_distinct(alt),\n    rows = n()\n  )\n\n# A tibble: 1 × 4\n  versions questions  alts  rows\n     &lt;int&gt;     &lt;int&gt; &lt;int&gt; &lt;int&gt;\n1      295        12     2  7080\n\n\nTo illustrate, this data shows that respondent 4 (a 19-year-old female, as seen in responses) saw this question in the first task (question == 1):\n\n\n\n\n    \n\n    \n    \n      First conjoint task presented to respondent 4\n        \n        \n              \n                 \n                Alternative 1\n                Alternative 2\n              \n        \n        \n        \n                \n                  Price    \n                  $3               \n                  $2             \n                \n                \n                  Packaging\n                  Plastic + sticker\n                  Plastic + paper\n                \n                \n                  Flavor   \n                  Chocolate        \n                  Nuts           \n                \n        \n      \n    \n\n\n\n…and she saw this question in the third task (question == 3):\n\n\n\n\n    \n\n    \n    \n      Third conjoint task presented to respondent 4\n        \n        \n              \n                 \n                Alternative 1\n                Alternative 2\n              \n        \n        \n        \n                \n                  Price    \n                  $4             \n                  $2               \n                \n                \n                  Packaging\n                  Plastic + paper\n                  Plastic + sticker\n                \n                \n                  Flavor   \n                  Chocolate      \n                  Chocolate        \n                \n        \n      \n    \n\n\n\n…and so on for 12 total tasks.\nFrom the respondent-level data, we know which alternative she chose: in the first question, she selected the first column (since responses$CBC_Random1 == 1)\n\n\n\n\n    \n\n    \n    \n      First conjoint task presented to respondent 4 with selected\nalternative highlighted\n        \n        \n              \n                 \n                Alternative 1\n                Alternative 2\n              \n        \n        \n        \n                \n                  Price    \n                  $3               \n                  $2             \n                \n                \n                  Packaging\n                  Plastic + sticker\n                  Plastic + paper\n                \n                \n                  Flavor   \n                  Chocolate        \n                  Nuts           \n                \n        \n      \n    \n\n\n\n…and in the third question she also selected the first column (since responses$CBC_Random3 == 1).\n\n\n\n\n    \n\n    \n    \n      Third conjoint task presented to respondent 4 with selected\nalternative highlighted\n        \n        \n              \n                 \n                Alternative 1\n                Alternative 2\n              \n        \n        \n        \n                \n                  Price    \n                  $4             \n                  $2               \n                \n                \n                  Packaging\n                  Plastic + paper\n                  Plastic + sticker\n                \n                \n                  Flavor   \n                  Chocolate      \n                  Chocolate",
    "crumbs": [
      "Fielding",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Clean and join data</span>"
    ]
  },
  {
    "objectID": "process-reshape-data.html#pivoting-expanding-and-combining",
    "href": "process-reshape-data.html#pivoting-expanding-and-combining",
    "title": "6  Clean and join data",
    "section": "6.3 Pivoting, expanding, and combining",
    "text": "6.3 Pivoting, expanding, and combining\nIn order to analyze this data with regression, we need to combine it into one long dataset, with a row for each respondent-choice.\nFirst, we take the wide responses data and pivot it longer so that there is a row per question per respondent (or 295 × 12, or 3,540 rows):\n\nresponses_long &lt;- responses |&gt; \n  pivot_longer(\n    cols = starts_with(\"CBC_Random\"),\n    names_to = \"question_raw\",\n    values_to = \"chosen_alt\"\n  ) |&gt; \n  # The task number is embedded in text, like \"CBC_Random6\"; this extracts it\n  mutate(question = as.numeric(str_extract(question_raw, \"\\\\d+\"))) |&gt; \n  select(-question_raw)\nresponses_long\n\n# A tibble: 3,540 × 5\n   resp_id gender   age chosen_alt question\n     &lt;dbl&gt; &lt;chr&gt;  &lt;dbl&gt;      &lt;dbl&gt;    &lt;dbl&gt;\n 1       4 Female    19          1        1\n 2       4 Female    19          2        2\n 3       4 Female    19          1        3\n 4       4 Female    19          1        4\n 5       4 Female    19          1        5\n 6       4 Female    19          1        6\n 7       4 Female    19          2        7\n 8       4 Female    19          1        8\n 9       4 Female    19          1        9\n10       4 Female    19          1       10\n# ℹ 3,530 more rows\n\n\n\nresponses_long |&gt; \n  summarize(\n    resp_ids = n_distinct(resp_id),\n    questions = n_distinct(question)\n  )\n\n# A tibble: 1 × 2\n  resp_ids questions\n     &lt;int&gt;     &lt;int&gt;\n1      295        12\n\n\nNext, we expand the long data so that there are rows for each of the two choices within each question, resulting in 295 × 12 × 2, or 7,080 rows, matching the alternatives data:\n\nresponses_long_expanded &lt;- responses_long |&gt;\n  expand(resp_id, question, alt = 1:2) |&gt; \n  left_join(responses_long, by = join_by(resp_id, question))\nresponses_long_expanded\n\n# A tibble: 7,080 × 6\n   resp_id question   alt gender   age chosen_alt\n     &lt;dbl&gt;    &lt;dbl&gt; &lt;int&gt; &lt;chr&gt;  &lt;dbl&gt;      &lt;dbl&gt;\n 1       4        1     1 Female    19          1\n 2       4        1     2 Female    19          1\n 3       4        2     1 Female    19          2\n 4       4        2     2 Female    19          2\n 5       4        3     1 Female    19          1\n 6       4        3     2 Female    19          1\n 7       4        4     1 Female    19          1\n 8       4        4     2 Female    19          1\n 9       4        5     1 Female    19          1\n10       4        5     2 Female    19          1\n# ℹ 7,070 more rows\n\n\n\nresponses_long_expanded |&gt; \n  summarize(\n    resp_ids = n_distinct(resp_id),\n    questions = n_distinct(question),\n    alts = n_distinct(alt),\n    rows = n()\n  )\n\n# A tibble: 1 × 4\n  resp_ids questions  alts  rows\n     &lt;int&gt;     &lt;int&gt; &lt;int&gt; &lt;int&gt;\n1      295        12     2  7080\n\n\nFinally, we join the alternative-level data to the long respondent-level data. We now have respondent-level characteristics and alternative-level characteristics in the same long data:\n\ncombined &lt;- responses_long_expanded |&gt; \n  left_join(alternatives, by = join_by(resp_id == version, question, alt)) |&gt; \n  mutate(choice = as.numeric(alt == chosen_alt))\ncombined\n\n# A tibble: 7,080 × 10\n   resp_id question   alt gender   age chosen_alt price packaging  flavor choice\n     &lt;dbl&gt;    &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;  &lt;dbl&gt;      &lt;dbl&gt; &lt;fct&gt; &lt;fct&gt;      &lt;fct&gt;   &lt;dbl&gt;\n 1       4        1     1 Female    19          1 $3    Plastic +… Choco…      1\n 2       4        1     2 Female    19          1 $2    Plastic +… Nuts        0\n 3       4        2     1 Female    19          2 $3    Plastic +… Nuts        0\n 4       4        2     2 Female    19          2 $2    Plastic +… Choco…      1\n 5       4        3     1 Female    19          1 $4    Plastic +… Choco…      1\n 6       4        3     2 Female    19          1 $2    Plastic +… Choco…      0\n 7       4        4     1 Female    19          1 $2    Plastic +… Choco…      1\n 8       4        4     2 Female    19          1 $4    Plastic +… Nuts        0\n 9       4        5     1 Female    19          1 $4    Plastic +… Choco…      1\n10       4        5     2 Female    19          1 $2    Plastic +… Nuts        0\n# ℹ 7,070 more rows",
    "crumbs": [
      "Fielding",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Clean and join data</span>"
    ]
  },
  {
    "objectID": "mms-amces-ols.html",
    "href": "mms-amces-ols.html",
    "title": "7  MMs and AMCEs with OLS",
    "section": "",
    "text": "7.1 Model\nWhen estimating causal effects, the main estimand of interest is an average treatment effect, or \\(E(Y \\mid X)\\). In political science and economics, analysts typically rely on an ordinary least squares (OLS) estimator, or a linear probability model (LPM), where the \\(\\beta\\) term in a linear regression model (or a partial derivative or marginal effect, if interaction terms are involved) represents the average effect of the treatment on the outcome. For those trained in predictive modeling or for Bayesians who seek out distributional families that reflect the underlying data generating process of the outcome variable, using LPMs on a binary (or multinomial) outcome can feel wrong. However, @Gomila:2021 demonstrates that in experiments with binary outcomes, LPMs are typically more consistent and unbiased than logistic regression estimators.\nThis means that analyzing conjoint data can be as simple as a basic linear model with lm().\nmodel_ols &lt;- lm(\n  choice ~ price + packaging + flavor,\n  data = stickers\n)\n\nmodel_parameters(model_ols, verbose = FALSE)\n\nParameter                     | Coefficient |   SE |         95% CI | t(7075) |      p\n--------------------------------------------------------------------------------------\n(Intercept)                   |        0.80 | 0.01 | [ 0.78,  0.82] |   69.39 | &lt; .001\nprice [$3]                    |       -0.14 | 0.01 | [-0.17, -0.12] |  -11.38 | &lt; .001\nprice [$4]                    |       -0.32 | 0.01 | [-0.34, -0.29] |  -25.23 | &lt; .001\npackaging [Plastic + sticker] |        0.11 | 0.01 | [ 0.09,  0.14] |   11.15 | &lt; .001\nflavor [Nuts]                 |       -0.41 | 0.01 | [-0.43, -0.39] |  -39.46 | &lt; .001\nWhile it is tempting (and possible) to determine the marginal means and causal effects from these raw regression coefficients—i.e., the intercept represents the average probability of selecting a $2 granola bar when all other characteristics are set to their reference values—it is more advisable to use the post-estimation functions from the {marginaleffects} package to calculate average predictions and comparisons. {marginaleffects} can provide probability-scale averages and contrasts, can calculate marginal means and effects across a balanced grid of attributed levels, and can adjust the estimated standard errors to account for repeated respondents.",
    "crumbs": [
      "Analyzing: causal effects",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>MMs and AMCEs with OLS</span>"
    ]
  },
  {
    "objectID": "mms-amces-ols.html#marginal-means",
    "href": "mms-amces-ols.html#marginal-means",
    "title": "7  MMs and AMCEs with OLS",
    "section": "7.2 Marginal means",
    "text": "7.2 Marginal means\nMarginal means represent the probability-scale fitted values from the model, calculated across a balanced reference grid of all possible combinations of feature levels. These predictions are then marginalized or averaged across features of interest.\nIn the case of the sticker experiment, there are 12 possible combinations of price, packaging, and flavor:\n\nfeature_grid &lt;- stickers |&gt; \n  tidyr::expand(price, packaging, flavor)\ntt(feature_grid)\n\n\n\n    \n\n    \n    \n      \n        \n        \n              \n                price\n                packaging\n                flavor\n              \n        \n        \n        \n                \n                  $2\n                  Plastic + paper\n                  Chocolate\n                \n                \n                  $2\n                  Plastic + paper\n                  Nuts\n                \n                \n                  $2\n                  Plastic + sticker\n                  Chocolate\n                \n                \n                  $2\n                  Plastic + sticker\n                  Nuts\n                \n                \n                  $3\n                  Plastic + paper\n                  Chocolate\n                \n                \n                  $3\n                  Plastic + paper\n                  Nuts\n                \n                \n                  $3\n                  Plastic + sticker\n                  Chocolate\n                \n                \n                  $3\n                  Plastic + sticker\n                  Nuts\n                \n                \n                  $4\n                  Plastic + paper\n                  Chocolate\n                \n                \n                  $4\n                  Plastic + paper\n                  Nuts\n                \n                \n                  $4\n                  Plastic + sticker\n                  Chocolate\n                \n                \n                  $4\n                  Plastic + sticker\n                  Nuts\n                \n        \n      \n    \n\n\n\nWe can feed each row of this balanced grid into the model to generate 12 predicted values:\n\npredictions(model_ols, newdata = feature_grid)\n\n\n Estimate Std. Error     z Pr(&gt;|z|)     S  2.5 % 97.5 %\n   0.8003     0.0115 69.39   &lt;0.001   Inf 0.7777 0.8229\n   0.3933     0.0115 34.10   &lt;0.001 844.2 0.3707 0.4159\n   0.9153     0.0115 79.39   &lt;0.001   Inf 0.8927 0.9379\n   0.5082     0.0115 44.07   &lt;0.001   Inf 0.4856 0.5308\n   0.6565     0.0115 56.91   &lt;0.001   Inf 0.6339 0.6791\n   0.2495     0.0115 21.63   &lt;0.001 342.4 0.2269 0.2721\n   0.7715     0.0115 66.88   &lt;0.001   Inf 0.7489 0.7941\n   0.3645     0.0115 31.59   &lt;0.001 725.3 0.3419 0.3871\n   0.4815     0.0115 41.73   &lt;0.001   Inf 0.4589 0.5042\n   0.0745     0.0115  6.46   &lt;0.001  33.2 0.0519 0.0971\n   0.5965     0.0115 51.71   &lt;0.001   Inf 0.5739 0.6191\n   0.1895     0.0115 16.43   &lt;0.001 199.2 0.1669 0.2121\n\nType: response\n\n\nFinally, we can marginalize or average these predicted values across features of interest. For instance, to find the marginal means for the two packaging conditions, we can calculate the group averages for the two types of packaging:\n\npredictions(model_ols, newdata = feature_grid) |&gt; \n  group_by(packaging) |&gt; \n  summarize(avg = mean(estimate))\n\n# A tibble: 2 × 2\n  packaging           avg\n  &lt;fct&gt;             &lt;dbl&gt;\n1 Plastic + paper   0.443\n2 Plastic + sticker 0.558\n\n\nManually creating a balanced reference grid and using group_by() and summarize() is useful for understanding the intuition behind finding estimated marginal means, but in practice it is better to use avg_predictions() from {marginaleffects}, which (1) creates the balanced grid automatically, (2) provides standard errors and other estimates of uncertainty, and (3) can adjust the standard errors to account for repeated respondents:\n\navg_predictions(\n  model_ols,\n  newdata = \"balanced\",\n  by = \"packaging\",\n  vcov = ~resp_id\n)\n\n\n         packaging Estimate Std. Error    z Pr(&gt;|z|)   S 2.5 % 97.5 %\n Plastic + paper      0.443    0.00868 51.0   &lt;0.001 Inf 0.426  0.460\n Plastic + sticker    0.558    0.00881 63.3   &lt;0.001 Inf 0.540  0.575\n\nType: response\n\n\nWe can calculate the marginal means individually for each conjoint feature, then combine them all into one large data frame for plotting and table-making.\n\nmm_ols &lt;- c(\"price\", \"packaging\", \"flavor\") |&gt;\n  set_names(str_to_title) |&gt;\n  map(\\(x) {\n    avg_predictions(\n      model_ols,\n      newdata = \"balanced\",\n      by = x,\n      vcov = ~resp_id\n    ) |&gt;\n      rename(attribute = all_of(x))\n  }) |&gt;\n  list_rbind(names_to = \"feature\")\n\n\nggplot(\n  mm_ols,\n  aes(x = estimate, y = fct_rev(attribute), color = feature)\n) +\n  geom_vline(xintercept = 0.5) +\n  geom_pointrange(aes(xmin = conf.low, xmax = conf.high)) +\n  scale_x_continuous(labels = label_percent()) +\n  guides(color = \"none\") +\n  facet_col(vars(feature), scales = \"free_y\", space = \"free\")",
    "crumbs": [
      "Analyzing: causal effects",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>MMs and AMCEs with OLS</span>"
    ]
  },
  {
    "objectID": "mms-amces-ols.html#average-marginal-component-effects-amces",
    "href": "mms-amces-ols.html#average-marginal-component-effects-amces",
    "title": "7  MMs and AMCEs with OLS",
    "section": "7.3 Average marginal component effects (AMCEs)",
    "text": "7.3 Average marginal component effects (AMCEs)\n\namce_ols &lt;- avg_comparisons(\n  model_ols,\n  newdata = \"balanced\",\n  vcov = ~resp_id\n)\namce_ols\n\n\n      Term                            Contrast Estimate Std. Error      z Pr(&gt;|z|)     S   2.5 % 97.5 %\n flavor    Nuts - Chocolate                      -0.407     0.0226 -18.04   &lt;0.001 239.2 -0.4513 -0.363\n packaging Plastic + sticker - Plastic + paper    0.115     0.0174   6.61   &lt;0.001  34.6  0.0809  0.149\n price     $3 - $2                               -0.144     0.0131 -10.99   &lt;0.001  90.9 -0.1694 -0.118\n price     $4 - $2                               -0.319     0.0187 -17.09   &lt;0.001 215.1 -0.3553 -0.282\n\nType: response\n\n\n\namces_split &lt;- amce_ols |&gt;\n  separate_wider_delim(\n    contrast, \n    delim = \" - \", \n    names = c(\"attribute\", \"reference_level\")\n  )\n\nreference_categories &lt;- amces_split |&gt;\n  distinct(term, reference_level) |&gt;\n  rename(attribute = reference_level) |&gt;\n  mutate(estimate = 0, conf.low = 0, conf.high = 0)\n\namces_split |&gt;\n  bind_rows(reference_categories) |&gt; \n  ggplot(aes(x = estimate, y = fct_rev(attribute), color = term)) +\n  geom_vline(xintercept = 0) +\n  geom_pointrange(aes(xmin = conf.low, xmax = conf.high)) +\n  scale_x_continuous(labels = label_percent()) +\n  guides(color = \"none\") +\n  facet_col(vars(term), scales = \"free_y\", space = \"free\")",
    "crumbs": [
      "Analyzing: causal effects",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>MMs and AMCEs with OLS</span>"
    ]
  },
  {
    "objectID": "mms-amces-ols.html#average-feature-choice-probability-afcp",
    "href": "mms-amces-ols.html#average-feature-choice-probability-afcp",
    "title": "7  MMs and AMCEs with OLS",
    "section": "7.4 Average Feature Choice Probability (AFCP)",
    "text": "7.4 Average Feature Choice Probability (AFCP)\nMaybe?",
    "crumbs": [
      "Analyzing: causal effects",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>MMs and AMCEs with OLS</span>"
    ]
  },
  {
    "objectID": "mms-amces-multinomial.html",
    "href": "mms-amces-multinomial.html",
    "title": "8  MMs and AMCEs with frequentist multinomial regression",
    "section": "",
    "text": "8.1 Model\nIt is also possible to use a multinomial logistic regression model that matches the distribution of the choice outcome variable. This can be done with a variety of R packages, including {mlogit}, {mclogit}, {logitr}, and {nnet}. Each behave slightly differently, requiring modifications to the data structure or needing additional post-processing work.\n{mlogit} is the oldest and most commonly used multinomial regression package and is the basis for many conjoint textbooks [@Feit], so I’ll illustrate how to use it to calculate MMs and AMCEs. {marginaleffects} does not support {mlogit} models because of the idiosyncracies its prediction functions, so the process requires a little manual work.",
    "crumbs": [
      "Analyzing: causal effects",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>MMs and AMCEs with frequentist multinomial regression</span>"
    ]
  },
  {
    "objectID": "mms-amces-multinomial.html#model",
    "href": "mms-amces-multinomial.html#model",
    "title": "8  MMs and AMCEs with frequentist multinomial regression",
    "section": "",
    "text": "Package\nData restructuring\nSupports {marginaleffects}?\nAllows for random effects?\n\n\n\n\n{mlogit}\nRequires an indexed data frame made with dfidx()\nNo\nYes, with rpar argument\n\n\n{mclogit}\nRequires a unique choice ID index\nYes\nYes, with random argument\n\n\n{logitr}\nRequires a unique choice ID index\nNo\nYes, with randPars argument\n\n\n{nnet}\nNone\nYes\nNo\n\n\n\n\n{mlogit}{mclogit}{logitr}{nnet}\n\n\n{mlogit} needs to work with an indexed data frame (created with dfidx()) that keeps track of the nested choices within respondents:\n\nstickers_indexed &lt;- stickers |&gt; \n  group_by(resp_id, question) |&gt; \n  mutate(choice_id = cur_group_id()) |&gt; \n  ungroup() |&gt; \n  as.data.frame() |&gt;  # mlogit() complains and breaks when working with tibbles :(\n  dfidx(\n    idx = list(c(\"choice_id\", \"resp_id\"), \"alt\"),\n    choice = \"choice\"\n  )\n\nIt then uses R’s standard formula syntax to define the model:\n\nmodel_mlogit &lt;- mlogit(\n  choice ~ price + packaging + flavor | 0,\n  data = stickers_indexed\n)\n\nmodel_parameters(model_mlogit, verbose = FALSE)\n\nParameter                     | Log-Odds |   SE |         95% CI |      z |      p\n----------------------------------------------------------------------------------\nprice [$3]                    |    -0.74 | 0.07 | [-0.87, -0.60] | -10.79 | &lt; .001\nprice [$4]                    |    -1.61 | 0.08 | [-1.76, -1.46] | -21.26 | &lt; .001\npackaging [Plastic + sticker] |     0.54 | 0.05 | [ 0.44,  0.64] |  10.24 | &lt; .001\nflavor [Nuts]                 |    -1.69 | 0.06 | [-1.81, -1.58] | -27.81 | &lt; .001\n\n\n\n\n{mclogit} does not need an official indexed data frame, but it does need a unique identifer for each possible choice:\n\nstickers_mclogit &lt;- stickers |&gt; \n  group_by(resp_id, question) |&gt; \n  mutate(choice_id = cur_group_id()) |&gt; \n  ungroup()\n\nhead(stickers_mclogit)\n\n# A tibble: 6 × 12\n  condition resp_id question   alt gender   age chosen_alt price packaging         flavor    choice choice_id\n  &lt;chr&gt;       &lt;dbl&gt;    &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;  &lt;dbl&gt;      &lt;dbl&gt; &lt;fct&gt; &lt;fct&gt;             &lt;fct&gt;      &lt;dbl&gt;     &lt;int&gt;\n1 Sticker         4        1     1 Female    19          1 $3    Plastic + sticker Chocolate      1         1\n2 Sticker         4        1     2 Female    19          1 $2    Plastic + paper   Nuts           0         1\n3 Sticker         4        2     1 Female    19          2 $3    Plastic + sticker Nuts           0         2\n4 Sticker         4        2     2 Female    19          2 $2    Plastic + paper   Chocolate      1         2\n5 Sticker         4        3     1 Female    19          1 $4    Plastic + paper   Chocolate      1         3\n6 Sticker         4        3     2 Female    19          1 $2    Plastic + sticker Chocolate      0         3\n\n\nWe can again use R’s formula syntax, but we need to specify two parts in the left-hand side: the binary choice variable and the set of choices it is nested in:\n\nlibrary(mclogit)\n\nmodel_mclogit &lt;- mclogit(\n  choice | choice_id ~ price + packaging + flavor,\n  data = stickers_mclogit\n)\n\n\nmodel_parameters(model_mclogit, verbose = FALSE)\n\nParameter                     | Log-Odds |   SE |         95% CI |      z |      p\n----------------------------------------------------------------------------------\nprice [$3]                    |    -0.74 | 0.07 | [-0.87, -0.60] | -10.79 | &lt; .001\nprice [$4]                    |    -1.61 | 0.08 | [-1.76, -1.46] | -21.26 | &lt; .001\npackaging [Plastic + sticker] |     0.54 | 0.05 | [ 0.44,  0.64] |  10.24 | &lt; .001\nflavor [Nuts]                 |    -1.69 | 0.06 | [-1.81, -1.58] | -27.81 | &lt; .001\n\n\n\n\n{logitr}\n\nstickers_logitr &lt;- stickers |&gt; \n  group_by(resp_id, question) |&gt; \n  mutate(choice_id = cur_group_id()) |&gt; \n  ungroup()\n\nhead(stickers_logitr)\n\n# A tibble: 6 × 12\n  condition resp_id question   alt gender   age chosen_alt price packaging         flavor    choice choice_id\n  &lt;chr&gt;       &lt;dbl&gt;    &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;  &lt;dbl&gt;      &lt;dbl&gt; &lt;fct&gt; &lt;fct&gt;             &lt;fct&gt;      &lt;dbl&gt;     &lt;int&gt;\n1 Sticker         4        1     1 Female    19          1 $3    Plastic + sticker Chocolate      1         1\n2 Sticker         4        1     2 Female    19          1 $2    Plastic + paper   Nuts           0         1\n3 Sticker         4        2     1 Female    19          2 $3    Plastic + sticker Nuts           0         2\n4 Sticker         4        2     2 Female    19          2 $2    Plastic + paper   Chocolate      1         2\n5 Sticker         4        3     1 Female    19          1 $4    Plastic + paper   Chocolate      1         3\n6 Sticker         4        3     2 Female    19          1 $2    Plastic + sticker Chocolate      0         3\n\n\n\nlibrary(logitr)\n\nmodel_logitr &lt;- logitr(\n  data = stickers_logitr,\n  outcome = \"choice\",\n  obsID = \"choice_id\",\n  pars = c(\"price\", \"packaging\", \"flavor\")\n)\n\n\nmodel_parameters(model_logitr, verbose = FALSE)\n\nParameter                     | Log-Odds |   SE |         95% CI |      z |      p\n----------------------------------------------------------------------------------\nprice [$3]                    |    -0.74 | 0.07 | [-0.87, -0.60] | -10.79 | &lt; .001\nprice [$4]                    |    -1.61 | 0.08 | [-1.76, -1.46] | -21.26 | &lt; .001\npackaging [Plastic + sticker] |     0.54 | 0.05 | [ 0.44,  0.64] |  10.24 | &lt; .001\nflavor [Nuts]                 |    -1.69 | 0.06 | [-1.81, -1.58] | -27.81 | &lt; .001\n\n\n\n\n{nnet} requires no indexing or question identifiers, which also means that it pools all the observations together and disregards the nested structure of the data.\n\nlibrary(nnet)\n\nmodel_nnet &lt;- multinom(\n  choice ~ price + packaging + flavor, \n  data = stickers\n)",
    "crumbs": [
      "Analyzing: causal effects",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>MMs and AMCEs with frequentist multinomial regression</span>"
    ]
  },
  {
    "objectID": "mms-amces-multinomial.html#marginal-means",
    "href": "mms-amces-multinomial.html#marginal-means",
    "title": "8  MMs and AMCEs with frequentist multinomial regression",
    "section": "8.2 Marginal means",
    "text": "8.2 Marginal means\nTo calculate marginal means, we need to generate predicted probabilities across a balanced grid of all conjoint features. This is a little trickier to do with multinomial {mlogit} models, though. {mlogit}’s predict() function requires that any new data passed to it include a row for each alternative (alt in the original data), since it will generate predictions for each alternative.\nFor example, if we only feed one combination of conjoint features to predict(), we’ll get an error:\n\nnewdata_example_bad &lt;- stickers |&gt; \n  slice(1) |&gt; \n  select(price, packaging, flavor)\nnewdata_example_bad\n\n# A tibble: 1 × 3\n  price packaging         flavor   \n  &lt;fct&gt; &lt;fct&gt;             &lt;fct&gt;    \n1 $3    Plastic + sticker Chocolate\n\npredict(model_mlogit, newdata = newdata_example_bad)\n\nError in predict.mlogit(model_mlogit, newdata = newdata_example_bad): the number of rows of the data.frame should be a multiple of the number of alternatives\n\n\nInstead, because respondents were presented with two alternatives at a time, we need to feed predict() a data frame with two alternatives.\n\n# The first two questions seen by two respondents\nnewdata_example &lt;- stickers |&gt; \n  filter(resp_id %in% c(4, 6), question == 1) |&gt; \n  select(resp_id, price, packaging, flavor, alt)\nnewdata_example\n\n# A tibble: 4 × 5\n  resp_id price packaging         flavor      alt\n    &lt;dbl&gt; &lt;fct&gt; &lt;fct&gt;             &lt;fct&gt;     &lt;dbl&gt;\n1       4 $3    Plastic + sticker Chocolate     1\n2       4 $2    Plastic + paper   Nuts          2\n3       6 $4    Plastic + paper   Chocolate     1\n4       6 $2    Plastic + sticker Chocolate     2\n\npredict(model_mlogit, newdata = newdata_example)\n\n       1      2\n1 0.8173 0.1827\n2 0.1043 0.8957\n\n\npredict() returns a matrix of probabilities, with one row per respondent and one column for each alternative. In this case, respondent 4 had an 83% chance of choosing the $3 + sticker + chocolate alternative when presented alongside a $2 + paper + nuts alternative, while respondent 6 had an 11% chance of choosing the $4 + paper + chocolate alternative when presented alongside a $2 + sticker + chocolate alternative.\nTo make a balanced grid of feature attributes, we need to create a grid of all 12 unique combinations (3 prices, 2 packagings, 2 flavors = 3 × 2 × 2 = 12) paired with evert other unique combination of features. This requires a bit of data manipulation, including cross_join() which combines each row from the 12-row feature grid with each row from itself, resulting in 144 (12 × 12) rows. We then remove the 12 rows where the two alternatives are identical, resulting in a grid of 132 possible pairs of alternatives:\n\nfeature_grid &lt;- stickers |&gt; \n  tidyr::expand(price, packaging, flavor)\n\n# Use cross_join to combine every row from the feature grid with itself\npaired_grid &lt;- feature_grid |&gt;\n  cross_join(\n    feature_grid |&gt; \n      rename(\n        price_alt2 = price,\n        packaging_alt2 = packaging,\n        flavor_alt2 = flavor\n      )\n  ) |&gt;\n  # Remove rows where both alternatives are identical\n  filter(\n    !(price == price_alt2 & \n      packaging == packaging_alt2 & \n      flavor == flavor_alt2)\n  ) |&gt;\n  # Create unique choice_id identifier\n  mutate(choice_id = row_number())\n\npaired_grid\n\n# A tibble: 132 × 7\n   price packaging       flavor    price_alt2 packaging_alt2    flavor_alt2 choice_id\n   &lt;fct&gt; &lt;fct&gt;           &lt;fct&gt;     &lt;fct&gt;      &lt;fct&gt;             &lt;fct&gt;           &lt;int&gt;\n 1 $2    Plastic + paper Chocolate $2         Plastic + paper   Nuts                1\n 2 $2    Plastic + paper Chocolate $2         Plastic + sticker Chocolate           2\n 3 $2    Plastic + paper Chocolate $2         Plastic + sticker Nuts                3\n 4 $2    Plastic + paper Chocolate $3         Plastic + paper   Chocolate           4\n 5 $2    Plastic + paper Chocolate $3         Plastic + paper   Nuts                5\n 6 $2    Plastic + paper Chocolate $3         Plastic + sticker Chocolate           6\n 7 $2    Plastic + paper Chocolate $3         Plastic + sticker Nuts                7\n 8 $2    Plastic + paper Chocolate $4         Plastic + paper   Chocolate           8\n 9 $2    Plastic + paper Chocolate $4         Plastic + paper   Nuts                9\n10 $2    Plastic + paper Chocolate $4         Plastic + sticker Chocolate          10\n# ℹ 122 more rows\n\n\n{mlogit} requires long data for predictions, so we can stack the two alternatives on top of each other, resulting in a data frame with 264 rows (132 × 2):\n\npaired_grid_long &lt;- bind_rows(\n  # Alternative 1\n  paired_grid |&gt;\n    select(choice_id, price, packaging, flavor) |&gt;\n    mutate(alt = 1),\n  # Alternative 2\n  paired_grid |&gt;\n    select(\n      choice_id,\n      price = price_alt2, \n      packaging = packaging_alt2,\n      flavor = flavor_alt2\n    ) |&gt;\n    mutate(alt = 2)\n) |&gt;\n  arrange(choice_id, alt)\npaired_grid_long\n\n# A tibble: 264 × 5\n   choice_id price packaging         flavor      alt\n       &lt;int&gt; &lt;fct&gt; &lt;fct&gt;             &lt;fct&gt;     &lt;dbl&gt;\n 1         1 $2    Plastic + paper   Chocolate     1\n 2         1 $2    Plastic + paper   Nuts          2\n 3         2 $2    Plastic + paper   Chocolate     1\n 4         2 $2    Plastic + sticker Chocolate     2\n 5         3 $2    Plastic + paper   Chocolate     1\n 6         3 $2    Plastic + sticker Nuts          2\n 7         4 $2    Plastic + paper   Chocolate     1\n 8         4 $3    Plastic + paper   Chocolate     2\n 9         5 $2    Plastic + paper   Chocolate     1\n10         5 $3    Plastic + paper   Nuts          2\n# ℹ 254 more rows\n\n\nFinally, we can generate predictions with this long data frame of all pairs of all combinations:\n\npredictions &lt;- predict(model_mlogit, newdata = paired_grid_long)\nhead(predictions)\n\n       1       2\n1 0.8449 0.15514\n2 0.3680 0.63198\n3 0.7603 0.23973\n4 0.6765 0.32351\n5 0.9193 0.08072\n6 0.5491 0.45092\n\n\npredict() returns a matrix with 2 columns, but we’re only really interested in one of them—we have a balanced grid of all possible pairs and only need to look at one half of each pair.\nWe can collapse this set of 132 predictions into the original balanced 12-row grid by calculating group specific means for price, packing, and flavor:\n\npreds_grid_mlogit &lt;- paired_grid_long |&gt;\n  filter(alt == 1) |&gt;\n  mutate(probability = predictions[,1]) |&gt;\n  group_by(price, packaging, flavor) |&gt;\n  summarize(estimate = mean(probability))\n\n`summarise()` has grouped output by 'price', 'packaging'. You can override using the `.groups` argument.\n\npreds_grid_mlogit\n\n# A tibble: 12 × 4\n# Groups:   price, packaging [6]\n   price packaging         flavor    estimate\n   &lt;fct&gt; &lt;fct&gt;             &lt;fct&gt;        &lt;dbl&gt;\n 1 $2    Plastic + paper   Chocolate    0.770\n 2 $2    Plastic + paper   Nuts         0.428\n 3 $2    Plastic + sticker Chocolate    0.853\n 4 $2    Plastic + sticker Nuts         0.544\n 5 $3    Plastic + paper   Chocolate    0.631\n 6 $3    Plastic + paper   Nuts         0.281\n 7 $3    Plastic + sticker Chocolate    0.735\n 8 $3    Plastic + sticker Nuts         0.387\n 9 $4    Plastic + paper   Chocolate    0.446\n10 $4    Plastic + paper   Nuts         0.141\n11 $4    Plastic + sticker Chocolate    0.562\n12 $4    Plastic + sticker Nuts         0.222\n\n\nFinally, we calculate marginal means from this new grid by marginalizing or averaging across specific features of interest:\n\npreds_grid_mlogit |&gt; \n  group_by(packaging) |&gt; \n  summarize(avg = mean(estimate))\n\n# A tibble: 2 × 2\n  packaging           avg\n  &lt;fct&gt;             &lt;dbl&gt;\n1 Plastic + paper   0.449\n2 Plastic + sticker 0.551\n\n\n\npaired_grid_long |&gt;\n  filter(alt == 1) |&gt;\n  mutate(probability = predictions[,1]) |&gt;\n  group_by(packaging) |&gt;\n  summarize(estimate = mean(probability))\n\n# A tibble: 2 × 2\n  packaging         estimate\n  &lt;fct&gt;                &lt;dbl&gt;\n1 Plastic + paper      0.449\n2 Plastic + sticker    0.551",
    "crumbs": [
      "Analyzing: causal effects",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>MMs and AMCEs with frequentist multinomial regression</span>"
    ]
  },
  {
    "objectID": "mms-amces-multinomial.html#what-about-standard-errors-and-confidence-intervals",
    "href": "mms-amces-multinomial.html#what-about-standard-errors-and-confidence-intervals",
    "title": "8  MMs and AMCEs with frequentist multinomial regression",
    "section": "8.3 What about standard errors and confidence intervals?",
    "text": "8.3 What about standard errors and confidence intervals?\nWe can calculate marginal means, but so far we can only get averages and not any measures of uncertainty. With OLS, we were able to use {marginaleffects} to find both means and standard errors. Because of how {mlogit} deals with predictions, {marginaleffects} does not support it—if you try to feed an {mlogit}-based model into one of {marginaleffects}’s functions, you’ll get this deprecation error:\n\nSupport for mlogit models was deprecated in version 0.23.0. The reason is that the data structure for these models is one observation-choice per row. Every other model-fitting package supported by marginaleffects treats rows as individual observations. The observation-choice structure made it harder to track indices and match individual predictions to rows in the original data. This added a lot of complexity to marginaleffects, and the results were not always reliable or safe.\n\nThe two most popular post-estimation packages—{marginaleffects} and {emmeans}—both struggle with multinomial models due to how they structure repeated data. nnet::multinom() is the only non-Bayesian multinomial package supported by both {marginaleffects} and {emmeans}, but as seen previously, it does not account for nested questions inside respondents.\nWe can measure the uncertainty of marginal means in a couple different ways:\n\nThe delta method, which requires manual matrix mulitplication and calculus to determine the gradient of the multinomial logistic function\nBootstrapping, which requires fitting hundreds of models on random subsets of the original data\n\nDetermining the gradient for the multinomial logistic distribution, especially with {mlogit}’s unique internal structuring of data, is surprisingly difficult. Again, neither {marginaleffects} nor {emmeans} can do it—{marginaleffects} tried for a while but gave up.\nBootstrapping, however, is a more flexible approach that requires no additional math or calculus, and it is farily straightforward with the {rsample} package. We can follow this general procedure:\n\nRandomly resample the original data with replacement some number of times\nRun the mlogit() model on each new sample\nAggregate the results from each model, using the mean and the 2.5% and 97.5% percentiles for a confidence interval\n\n\n\n\n\n\n\nBootstrapping is flexible!\n\n\n\nThe example below uses {mlogit}, but the same approach will work for any of the multinomial logistic regression packages. As long as you can fit a model and generate predicted probabilities with it, you can repeat that process over and over on different versions of your data to simulate a confidence interval.\n\n\n\n8.3.1 Randomly resample the original data\nFirst we’ll use bootstraps() to randomly resample the original data 1,000 times. With data that lacks a nested structure, this is as straightforward as running bootstraps(name_of_data, times = 1000). However, if we do that here, pairs of questions will be separated. Every respondent has 24 rows: 2 alternatives across 12 questions. We need to keep this respondent-level data together when resampling.\nOne way to ensure this happens is to group by respondent ID and then nest the remaining data into a list column:\n\nstickers |&gt;\n  group_by(resp_id) |&gt; \n  nest()\n\n# A tibble: 295 × 2\n# Groups:   resp_id [295]\n   resp_id data              \n     &lt;dbl&gt; &lt;list&gt;            \n 1       4 &lt;tibble [24 × 10]&gt;\n 2       5 &lt;tibble [24 × 10]&gt;\n 3       6 &lt;tibble [24 × 10]&gt;\n 4       7 &lt;tibble [24 × 10]&gt;\n 5       8 &lt;tibble [24 × 10]&gt;\n 6       9 &lt;tibble [24 × 10]&gt;\n 7      10 &lt;tibble [24 × 10]&gt;\n 8      11 &lt;tibble [24 × 10]&gt;\n 9      12 &lt;tibble [24 × 10]&gt;\n10      13 &lt;tibble [24 × 10]&gt;\n# ℹ 285 more rows\n\n\nWhen {rsample} randomly resamples this data, it will keep the data associated with each respondent:\n\nlibrary(rsample)\n\nset.seed(841630)  # From random.org\n\nbootstrapped_stickers &lt;- stickers |&gt;\n  group_by(resp_id) |&gt; \n  nest() |&gt; \n  ungroup() |&gt; \n  bootstraps(\n    times = 1000\n  )\n\nWe can confirm this if we look at one of the bootstrapped samples. Each respondent still has their associated data:\n\nbootstrapped_stickers$splits[[1]] |&gt; analysis()\n\n# A tibble: 295 × 2\n   resp_id data              \n     &lt;dbl&gt; &lt;list&gt;            \n 1      49 &lt;tibble [24 × 10]&gt;\n 2     170 &lt;tibble [24 × 10]&gt;\n 3      11 &lt;tibble [24 × 10]&gt;\n 4     102 &lt;tibble [24 × 10]&gt;\n 5     177 &lt;tibble [24 × 10]&gt;\n 6     254 &lt;tibble [24 × 10]&gt;\n 7     153 &lt;tibble [24 × 10]&gt;\n 8       5 &lt;tibble [24 × 10]&gt;\n 9     289 &lt;tibble [24 × 10]&gt;\n10     298 &lt;tibble [24 × 10]&gt;\n# ℹ 285 more rows\n\n\n\n\n8.3.2 Run the model on each sample\nNext, we need to run mlogit() on each bootstrapped data frame. We already have the code for creating an indexed data frame, running a model, and generating predictions—we’ll wrap all that up into a more general function:\n\nfit_predict_mlogit &lt;- function(.split, feature_grid, ...) {\n  .df &lt;- as.data.frame(.split) |&gt; \n    # Assign new unique respondent IDs (since some will be repeated through the\n    # bootstrapping process), and index the bootstrapped data frame so that it\n    # works with mlogit\n    mutate(resp_id = row_number()) |&gt; \n    # Unnest the respondent-specific data\n    unnest(data) |&gt; \n    group_by(resp_id, question) |&gt; \n    mutate(choice_id = cur_group_id()) |&gt; \n    ungroup() |&gt; \n    as.data.frame() |&gt; \n    dfidx(\n      idx = list(c(\"choice_id\", \"resp_id\"), \"alt\"),\n      choice = \"choice\",\n      shape = \"long\"\n    )\n\n  # Fit mlogit model\n  model &lt;- mlogit(\n    choice ~ price + packaging + flavor,\n    data = .df\n  )\n\n  # Generate predicted probabilities on balanced feature grid\n  predictions &lt;- predict(model, newdata = feature_grid)\n\n  # Aggregate predictions into feature-specific averages\n  feature_grid |&gt; \n    filter(alt == 1) |&gt; \n    mutate(estimate = predictions[, 1]) |&gt; \n    group_by(price, packaging, flavor) |&gt;\n    summarize(estimate = mean(estimate)) |&gt; \n    ungroup()\n}\n\nThen we’ll feed each bootstrapped sample into our fit_predict_mlogit() function. This will take a while!\n\nboot_results &lt;- bootstrapped_stickers |&gt;\n  mutate(boot_fits = map(splits, fit_predict_mlogit, feature_grid = paired_grid_long))\n\nWe now have a column with the average predicted probabilities for each of the 12 combinations of conjoint features for each bootstrapped dataset.\n\nboot_results\n\n# Bootstrap sampling \n# A tibble: 1,000 × 3\n   splits            id            boot_fits          \n   &lt;list&gt;            &lt;chr&gt;         &lt;list&gt;             \n 1 &lt;split [295/106]&gt; Bootstrap0001 &lt;gropd_df [12 × 4]&gt;\n 2 &lt;split [295/113]&gt; Bootstrap0002 &lt;gropd_df [12 × 4]&gt;\n 3 &lt;split [295/114]&gt; Bootstrap0003 &lt;gropd_df [12 × 4]&gt;\n 4 &lt;split [295/110]&gt; Bootstrap0004 &lt;gropd_df [12 × 4]&gt;\n 5 &lt;split [295/113]&gt; Bootstrap0005 &lt;gropd_df [12 × 4]&gt;\n 6 &lt;split [295/117]&gt; Bootstrap0006 &lt;gropd_df [12 × 4]&gt;\n 7 &lt;split [295/113]&gt; Bootstrap0007 &lt;gropd_df [12 × 4]&gt;\n 8 &lt;split [295/106]&gt; Bootstrap0008 &lt;gropd_df [12 × 4]&gt;\n 9 &lt;split [295/113]&gt; Bootstrap0009 &lt;gropd_df [12 × 4]&gt;\n10 &lt;split [295/111]&gt; Bootstrap0010 &lt;gropd_df [12 × 4]&gt;\n# ℹ 990 more rows\n\n\nHere’s what one looks like:\n\nboot_results$boot_fits[[1]]\n\n# A tibble: 12 × 4\n# Groups:   price, packaging [6]\n   price packaging         flavor    estimate\n   &lt;fct&gt; &lt;fct&gt;             &lt;fct&gt;        &lt;dbl&gt;\n 1 $2    Plastic + paper   Chocolate    0.763\n 2 $2    Plastic + paper   Nuts         0.428\n 3 $2    Plastic + sticker Chocolate    0.852\n 4 $2    Plastic + sticker Nuts         0.553\n 5 $3    Plastic + paper   Chocolate    0.611\n 6 $3    Plastic + paper   Nuts         0.270\n 7 $3    Plastic + sticker Chocolate    0.726\n 8 $3    Plastic + sticker Nuts         0.385\n 9 $4    Plastic + paper   Chocolate    0.472\n10 $4    Plastic + paper   Nuts         0.163\n11 $4    Plastic + sticker Chocolate    0.597\n12 $4    Plastic + sticker Nuts         0.258\n\n\n\n\n8.3.3 Aggregate the results from each model\nAs before, we can calculate marginal means by calculating group averages for the different conjoint features in this balanced reference grid. Since we’re working with 1,000 data frames instead of just 1, we’ll need to use map() to group and summarize. The estimate column here shows the marginal mean for each packaging condition in each of the bootstrapped samples:\n\nmms_packaging &lt;- boot_results |&gt; \n  mutate(mms = map(boot_fits, \\(.x) {\n    .x |&gt; \n      group_by(packaging) |&gt; \n      summarize(estimate = mean(estimate))\n  })) |&gt; \n  unnest(mms)\nmms_packaging\n\n# A tibble: 2,000 × 5\n   splits            id            boot_fits           packaging         estimate\n   &lt;list&gt;            &lt;chr&gt;         &lt;list&gt;              &lt;fct&gt;                &lt;dbl&gt;\n 1 &lt;split [295/106]&gt; Bootstrap0001 &lt;gropd_df [12 × 4]&gt; Plastic + paper      0.451\n 2 &lt;split [295/106]&gt; Bootstrap0001 &lt;gropd_df [12 × 4]&gt; Plastic + sticker    0.562\n 3 &lt;split [295/113]&gt; Bootstrap0002 &lt;gropd_df [12 × 4]&gt; Plastic + paper      0.451\n 4 &lt;split [295/113]&gt; Bootstrap0002 &lt;gropd_df [12 × 4]&gt; Plastic + sticker    0.563\n 5 &lt;split [295/114]&gt; Bootstrap0003 &lt;gropd_df [12 × 4]&gt; Plastic + paper      0.446\n 6 &lt;split [295/114]&gt; Bootstrap0003 &lt;gropd_df [12 × 4]&gt; Plastic + sticker    0.552\n 7 &lt;split [295/110]&gt; Bootstrap0004 &lt;gropd_df [12 × 4]&gt; Plastic + paper      0.449\n 8 &lt;split [295/110]&gt; Bootstrap0004 &lt;gropd_df [12 × 4]&gt; Plastic + sticker    0.552\n 9 &lt;split [295/113]&gt; Bootstrap0005 &lt;gropd_df [12 × 4]&gt; Plastic + paper      0.442\n10 &lt;split [295/113]&gt; Bootstrap0005 &lt;gropd_df [12 × 4]&gt; Plastic + sticker    0.564\n# ℹ 1,990 more rows\n\n\nWe can visualize the distribution of these marginal means:\n\nggplot(mms_packaging, aes(x = estimate, fill = packaging)) +\n  geom_histogram(color = \"white\") +\n  guides(fill = \"none\") +\n  facet_wrap(vars(packaging), ncol = 1)\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\n\nAnd we can calculate confidence intervals based on percentiles. We can either use quantile() manually, or we can use this custom function to get a cleaner, more complete summary of the intervals:\n\npercentile_ci &lt;- function(x, alpha = 0.05) {\n  x &lt;- na.omit(x)\n\n  lower &lt;- quantile(x, probs = alpha / 2)\n  upper &lt;- quantile(x, probs = 1 - alpha / 2)\n  estimate &lt;- mean(x)\n\n  tibble(\n    .lower = lower,\n    .estimate = estimate,\n    .upper = upper,\n    .alpha = alpha,\n    .method = \"percentile\"\n  )\n}\n\nmms_packaging |&gt; \n  group_by(packaging) |&gt; \n  summarize(details = percentile_ci(estimate)) |&gt; \n  unnest(details)\n\n# A tibble: 2 × 6\n  packaging         .lower .estimate .upper .alpha .method   \n  &lt;fct&gt;              &lt;dbl&gt;     &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;     \n1 Plastic + paper    0.438     0.458  0.478   0.05 percentile\n2 Plastic + sticker  0.539     0.559  0.580   0.05 percentile\n\n\nWe can calculate the marginal means individually for each conjoint feature, then combine them all into one large data frame for plotting and table-making.\n\nmms_all &lt;- boot_results |&gt; \n  mutate(mms_price = map(boot_fits, \\(.x) {\n    .x |&gt; \n      group_by(price) |&gt; \n      summarize(estimate = mean(estimate))\n  })) |&gt; \n  mutate(mms_packaging = map(boot_fits, \\(.x) {\n    .x |&gt; \n      group_by(packaging) |&gt; \n      summarize(estimate = mean(estimate))\n  })) |&gt; \n  mutate(mms_flavor = map(boot_fits, \\(.x) {\n    .x |&gt; \n      group_by(flavor) |&gt; \n      summarize(estimate = mean(estimate))\n  }))\n\nmm_price_boot &lt;- mms_all |&gt; \n  unnest(mms_price) |&gt; \n  group_by(attribute = price) |&gt; \n  summarize(details = percentile_ci(estimate)) |&gt; \n  unnest(details)\n\nmm_packaging_boot &lt;- mms_all |&gt; \n  unnest(mms_packaging) |&gt; \n  group_by(attribute = packaging) |&gt; \n  summarize(details = percentile_ci(estimate)) |&gt; \n  unnest(details)\n\nmm_flavor_boot &lt;- mms_all |&gt; \n  unnest(mms_flavor) |&gt; \n  group_by(attribute = flavor) |&gt; \n  summarize(details = percentile_ci(estimate)) |&gt; \n  unnest(details)\n\nmm_boot &lt;- bind_rows(list(\n  \"Price\" = mm_price_boot,\n  \"Packaging\" = mm_packaging_boot,\n  \"Flavor\" = mm_flavor_boot\n), .id = \"feature\") |&gt;\n  as_tibble()\n\n\nggplot(\n  mm_boot,\n  aes(x = .estimate, y = fct_rev(attribute), color = feature)\n) +\n  geom_vline(xintercept = 0.5) +\n  geom_pointrange(aes(xmin = .lower, xmax = .upper)) +\n  scale_x_continuous(labels = label_percent()) +\n  guides(color = \"none\") +\n  facet_col(vars(feature), scales = \"free_y\", space = \"free\")",
    "crumbs": [
      "Analyzing: causal effects",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>MMs and AMCEs with frequentist multinomial regression</span>"
    ]
  },
  {
    "objectID": "mms-amces-multinomial.html#average-marginal-component-effects-amces",
    "href": "mms-amces-multinomial.html#average-marginal-component-effects-amces",
    "title": "8  MMs and AMCEs with frequentist multinomial regression",
    "section": "8.4 Average marginal component effects (AMCEs)",
    "text": "8.4 Average marginal component effects (AMCEs)\nAverage marginal component effects (AMCEs) are differences in marginal means, where one attribute is used as a reference category. With OLS, we were able to calculate them automatically with marginaleffects::avg_comparisons(), but as seen above, {marginaleffects} can’t work with {mlogit}. We have a balanced grid of predicted probabilities, though, which means we can find the differences in means ourselves with a little data wrangling.\n\npreds_grid_mlogit |&gt; \n  group_by(price) |&gt; \n  summarize(mm = mean(estimate)) |&gt; \n  mutate(amce = mm - mm[price == \"$2\"])\n\n# A tibble: 3 × 3\n  price    mm   amce\n  &lt;fct&gt; &lt;dbl&gt;  &lt;dbl&gt;\n1 $2    0.649  0    \n2 $3    0.508 -0.140\n3 $4    0.343 -0.306\n\npreds_grid_mlogit |&gt; \n  group_by(packaging) |&gt; \n  summarize(mm = mean(estimate)) |&gt; \n  mutate(amce = mm - mm[packaging == \"Plastic + paper\"])\n\n# A tibble: 2 × 3\n  packaging            mm  amce\n  &lt;fct&gt;             &lt;dbl&gt; &lt;dbl&gt;\n1 Plastic + paper   0.449 0    \n2 Plastic + sticker 0.551 0.101\n\npreds_grid_mlogit |&gt; \n  group_by(flavor) |&gt; \n  summarize(mm = mean(estimate)) |&gt; \n  mutate(amce = mm - mm[flavor == \"Nuts\"])\n\n# A tibble: 2 × 3\n  flavor       mm  amce\n  &lt;fct&gt;     &lt;dbl&gt; &lt;dbl&gt;\n1 Chocolate 0.666 0.332\n2 Nuts      0.334 0    \n\n\nWe can go through the same process with the bootstrapped data as well to calculate the uncertainty for each AMCE:\n\namces_all &lt;- boot_results |&gt; \n  mutate(mms_price = map(boot_fits, \\(.x) {\n    .x |&gt; \n      group_by(price) |&gt; \n      summarize(estimate = mean(estimate)) |&gt; \n      mutate(amce = estimate - estimate[price == \"$2\"])\n  })) |&gt; \n  mutate(mms_packaging = map(boot_fits, \\(.x) {\n    .x |&gt; \n      group_by(packaging) |&gt; \n      summarize(estimate = mean(estimate)) |&gt; \n      mutate(amce = estimate - estimate[packaging == \"Plastic + paper\"])\n  })) |&gt; \n  mutate(mms_flavor = map(boot_fits, \\(.x) {\n    .x |&gt; \n      group_by(flavor) |&gt; \n      summarize(estimate = mean(estimate)) |&gt; \n      mutate(amce = estimate - estimate[flavor == \"Nuts\"])\n  }))\n\namces_price_boot &lt;- amces_all |&gt; \n  unnest(mms_price) |&gt; \n  group_by(attribute = price) |&gt; \n  summarize(details = percentile_ci(amce)) |&gt; \n  unnest(details)\n\namces_packaging_boot &lt;- amces_all |&gt; \n  unnest(mms_packaging) |&gt; \n  group_by(attribute = packaging) |&gt; \n  summarize(details = percentile_ci(amce)) |&gt; \n  unnest(details)\n\namces_flavor_boot &lt;- amces_all |&gt; \n  unnest(mms_flavor) |&gt; \n  group_by(attribute = flavor) |&gt; \n  summarize(details = percentile_ci(amce)) |&gt; \n  unnest(details)\n\namces_boot &lt;- bind_rows(list(\n  \"Price\" = amces_price_boot,\n  \"Packaging\" = amces_packaging_boot,\n  \"Flavor\" = amces_flavor_boot\n), .id = \"feature\") |&gt;\n  as_tibble()\n\n\nggplot(\n  amces_boot,\n  aes(x = .estimate, y = fct_rev(attribute), color = feature)\n) +\n  geom_vline(xintercept = 0) +\n  geom_pointrange(aes(xmin = .lower, xmax = .upper)) +\n  scale_x_continuous(labels = label_pp) +\n  guides(color = \"none\") +\n  facet_col(vars(feature), scales = \"free_y\", space = \"free\")\n\n\n\n\n\n\n\n\n\n\nmodel_logitr\n\n\nCall:\nlogitr(data = stickers_logitr, outcome = \"choice\", obsID = \"choice_id\",     pars = c(\"price\", \"packaging\", \"flavor\"))\n\nA Multinomial Logit model estimated in the Preference space\n\nExit Status: 3, Optimization stopped because ftol_rel or ftol_abs was reached.\n\nCoefficients:\n                   price$3                     price$4  packagingPlastic + sticker                  flavorNuts  \n                    -0.738                      -1.610                       0.541                      -1.695  \n[1] -1671\n\nalts &lt;- stickers |&gt;\n  tidyr::expand(price, packaging, flavor)\n\ncombn(nrow(alts), 2)\n\n     [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10] [,11] [,12] [,13] [,14] [,15] [,16] [,17] [,18] [,19] [,20] [,21] [,22] [,23] [,24] [,25] [,26] [,27] [,28] [,29] [,30] [,31] [,32] [,33] [,34]\n[1,]    1    1    1    1    1    1    1    1    1     1     1     2     2     2     2     2     2     2     2     2     2     3     3     3     3     3     3     3     3     3     4     4     4     4\n[2,]    2    3    4    5    6    7    8    9   10    11    12     3     4     5     6     7     8     9    10    11    12     4     5     6     7     8     9    10    11    12     5     6     7     8\n     [,35] [,36] [,37] [,38] [,39] [,40] [,41] [,42] [,43] [,44] [,45] [,46] [,47] [,48] [,49] [,50] [,51] [,52] [,53] [,54] [,55] [,56] [,57] [,58] [,59] [,60] [,61] [,62] [,63] [,64] [,65] [,66]\n[1,]     4     4     4     4     5     5     5     5     5     5     5     6     6     6     6     6     6     7     7     7     7     7     8     8     8     8     9     9     9    10    10    11\n[2,]     9    10    11    12     6     7     8     9    10    11    12     7     8     9    10    11    12     8     9    10    11    12     9    10    11    12    10    11    12    11    12    12\n\nchoose(12, 2)\n\n[1] 66\n\nn_alt &lt;- 2\n\nchoice_sets_long &lt;- alts |&gt;\n  (\\(x) combn(seq_len(nrow(x)), m = n_alt, simplify = FALSE))() |&gt;\n  map(\\(idx) {\n    alts[idx, ] |&gt;\n      mutate(obsID = cur_group_id(), alt = seq_len(n_alt))\n  }) |&gt;\n  list_rbind(names_to = \"obsID\") |&gt;\n  mutate(obsID = as.integer(obsID)) |&gt;\n  arrange(obsID, alt) |&gt;\n  as_tibble()\nchoice_sets_long\n\n# A tibble: 132 × 5\n   price packaging         flavor    obsID   alt\n   &lt;fct&gt; &lt;fct&gt;             &lt;fct&gt;     &lt;int&gt; &lt;int&gt;\n 1 $2    Plastic + paper   Chocolate     1     1\n 2 $2    Plastic + paper   Nuts          1     2\n 3 $2    Plastic + paper   Chocolate     2     1\n 4 $2    Plastic + sticker Chocolate     2     2\n 5 $2    Plastic + paper   Chocolate     3     1\n 6 $2    Plastic + sticker Nuts          3     2\n 7 $2    Plastic + paper   Chocolate     4     1\n 8 $3    Plastic + paper   Chocolate     4     2\n 9 $2    Plastic + paper   Chocolate     5     1\n10 $3    Plastic + paper   Nuts          5     2\n# ℹ 122 more rows\n\npreds_logitr &lt;- predict(\n  model_logitr,\n  newdata = choice_sets_long,\n  obsID = 'obsID',\n  returnData = TRUE\n)\n\npreds_logitr |&gt;\n  group_by(packaging) |&gt;\n  summarize(avg_pred = mean(predicted_prob))\n\n# A tibble: 2 × 2\n  packaging         avg_pred\n  &lt;fct&gt;                &lt;dbl&gt;\n1 Plastic + paper      0.449\n2 Plastic + sticker    0.551\n\nmms_logitr &lt;- list(\n  \"Price\" = \"price\",\n  \"Packaging\" = \"packaging\",\n  \"Flavor\" = \"flavor\"\n) |&gt;\n  map(\\(.x) {\n    preds_logitr |&gt;\n      group_by(attribute = .data[[.x]]) |&gt;\n      summarize(\n        estimate = mean(predicted_prob),\n        se = sd(predicted_prob) / sqrt(n()),\n        conf.low = estimate - 1.96 * se,\n        conf.high = estimate + 1.96 * se\n      )\n  }) |&gt;\n  list_rbind(names_to = \"feature\")\n\nggplot(\n  mms_logitr,\n  aes(x = estimate, y = fct_rev(attribute), color = feature)\n) +\n  geom_vline(xintercept = 0.5) +\n  geom_pointrange(aes(xmin = conf.low, xmax = conf.high)) +\n  scale_x_continuous(labels = label_percent()) +\n  guides(color = \"none\") +\n  facet_col(vars(feature), scales = \"free_y\", space = \"free\")",
    "crumbs": [
      "Analyzing: causal effects",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>MMs and AMCEs with frequentist multinomial regression</span>"
    ]
  },
  {
    "objectID": "mms-amces-multinomial.html#bootstrapping-with-logitr",
    "href": "mms-amces-multinomial.html#bootstrapping-with-logitr",
    "title": "8  MMs and AMCEs with frequentist multinomial regression",
    "section": "8.5 Bootstrapping with {logitr}",
    "text": "8.5 Bootstrapping with {logitr}\nPhew, this is a messy attempt\n\nlibrary(rsample)\n\nset.seed(734498) # From random.org\n\nbootstrapped_stickers_logitr &lt;- stickers |&gt;\n  group_by(resp_id) |&gt;\n  nest() |&gt;\n  ungroup() |&gt;\n  bootstraps(\n    times = 1000\n  )\n\nfit_logitr &lt;- function(.split) {\n  library(dplyr)\n  library(tidyr)\n  library(logitr)\n\n  .df &lt;- rsample::analysis(.split) |&gt;\n    # Assign new unique respondent IDs (since some will be repeated through the\n    # bootstrapping process)\n    mutate(resp_id = row_number()) |&gt;\n    # Unnest the respondent-specific data\n    unnest(data) |&gt;\n    group_by(resp_id, question) |&gt;\n    mutate(choice_id = cur_group_id()) |&gt;\n    ungroup()\n\n  # Fit logitr model\n  model_logitr &lt;- logitr(\n    data = .df,\n    outcome = \"choice\",\n    obsID = \"choice_id\",\n    pars = c(\"price\", \"packaging\", \"flavor\")\n  )\n\n  # # Generate predicted probabilities on balanced feature grid\n  # predictions &lt;- predict(model, newdata = feature_grid)\n\n  # # Aggregate predictions into feature-specific averages\n  # feature_grid |&gt;\n  #   filter(alt == 1) |&gt;\n  #   mutate(estimate = predictions[, 1]) |&gt;\n  #   group_by(price, packaging, flavor) |&gt;\n  #   summarize(estimate = mean(estimate)) |&gt;\n  #   ungroup()\n}\n\nmirai::daemons(10)\n\n[1] 10\n\nboot_results_purrr &lt;- bootstrapped_stickers_logitr |&gt;\n  # slice(1:10) |&gt;\n  mutate(\n    model = map(\n      splits,\n      in_parallel(\\(x) fit_logitr(x), fit_logitr = fit_logitr),\n      .progress = TRUE\n    )\n  )\n\n■■■                                7% | ETA: 14s\n\n\n■■■■                              11% | ETA: 21s\n\n\n■■■■■■■                           20% | ETA: 22s\n\n\n■■■■■■■■■■                        28% | ETA: 23s\n\n\n■■■■■■■■■■■■■                     39% | ETA: 19s\n\n\n■■■■■■■■■■■■■■                    44% | ETA: 19s\n\n\n■■■■■■■■■■■■■■■■■■                56% | ETA: 14s\n\n\n■■■■■■■■■■■■■■■■■■■■              63% | ETA: 12s\n\n\n■■■■■■■■■■■■■■■■■■■■■■■           72% | ETA:  9s\n\n\n■■■■■■■■■■■■■■■■■■■■■■■■■         81% | ETA:  6s\n\n\n■■■■■■■■■■■■■■■■■■■■■■■■■■■■      89% | ETA:  4s\n\n\n■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■    98% | ETA:  1s\n\n\n■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■  100% | ETA:  0s\n\nmirai::daemons(0)\n\n[1] 0\n\ncalc_mms_logitr &lt;- function(pr) {\n  list(\n    \"Price\" = \"price\",\n    \"Packaging\" = \"packaging\",\n    \"Flavor\" = \"flavor\"\n  ) |&gt;\n    map(\\(.x) {\n      pr |&gt;\n        group_by(attribute = .data[[.x]]) |&gt;\n        summarize(\n          estimate = mean(estimate)\n        )\n    }) |&gt;\n    list_rbind(names_to = \"feature\")\n}\n\n\nasdf &lt;- boot_results_purrr |&gt;\n  mutate(\n    preds = map(model, \\(.x) {\n      predict(\n        .x,\n        newdata = choice_sets_long,\n        obsID = 'obsID',\n        returnData = TRUE\n      ) |&gt; \n        group_by(price, packaging, flavor) |&gt;\n        summarize(estimate = mean(predicted_prob), .groups = \"drop\")\n    })\n  )\n\nasdfasdf &lt;- asdf |&gt; \n  mutate(mms = map(preds, \\(.preds) calc_mms_logitr(.preds)))\n\nasdfasdf |&gt; \n  unnest(mms) |&gt; \n  group_by(feature, attribute) |&gt; \n  summarize(details = percentile_ci(estimate)) |&gt; \n  unnest(details) |&gt; \n  ggplot(aes(x = .estimate, y = fct_rev(attribute), color = feature)) +\n  geom_vline(xintercept = 0.5) +\n  geom_pointrange(aes(xmin = .lower, xmax = .upper)) +\n  scale_x_continuous(labels = label_percent()) +\n  guides(color = \"none\") +\n  facet_col(vars(feature), scales = \"free_y\", space = \"free\")\n\n`summarise()` has grouped output by 'feature'. You can override using the `.groups` argument.",
    "crumbs": [
      "Analyzing: causal effects",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>MMs and AMCEs with frequentist multinomial regression</span>"
    ]
  },
  {
    "objectID": "mms-amces-bayes.html",
    "href": "mms-amces-bayes.html",
    "title": "9  MMs and AMCEs with Bayesian multinomial logistic regression",
    "section": "",
    "text": "blah blah\n\nlibrary(tidyverse)\nlibrary(brms)\nlibrary(rstan)\nlibrary(tidybayes)\nlibrary(ggdist)\nlibrary(marginaleffects)\nlibrary(parameters)\nlibrary(tinytable)\nlibrary(scales)\nlibrary(ggforce)\n\nstickers &lt;- readRDS(\"data/processed_data/study_5_sticker.rds\")\n\n\nstickers_choice_alt &lt;- stickers |&gt;\n  mutate(choice_alt = factor(alt * choice))\n\nstickers_choice_alt |&gt;\n  select(resp_id, question, price, packaging, flavor, choice, choice_alt)\n\n# A tibble: 7,080 × 7\n   resp_id question price packaging         flavor    choice choice_alt\n     &lt;dbl&gt;    &lt;dbl&gt; &lt;fct&gt; &lt;fct&gt;             &lt;fct&gt;      &lt;dbl&gt; &lt;fct&gt;     \n 1       4        1 $3    Plastic + sticker Chocolate      1 1         \n 2       4        1 $2    Plastic + paper   Nuts           0 0         \n 3       4        2 $3    Plastic + sticker Nuts           0 0         \n 4       4        2 $2    Plastic + paper   Chocolate      1 2         \n 5       4        3 $4    Plastic + paper   Chocolate      1 1         \n 6       4        3 $2    Plastic + sticker Chocolate      0 0         \n 7       4        4 $2    Plastic + sticker Chocolate      1 1         \n 8       4        4 $4    Plastic + paper   Nuts           0 0         \n 9       4        5 $4    Plastic + sticker Chocolate      1 1         \n10       4        5 $2    Plastic + paper   Nuts           0 0         \n# ℹ 7,070 more rows\n\n\n\n\\[\n\\begin{aligned}\n&\\ \\textbf{Multinomial probability of selection of choice}_i \\textbf{ in respondent}_j \\\\\n\\text{Choice}_{i_j} \\sim&\\ \\operatorname{Categorical}(\\{\\mu_{1,i_j}, \\mu_{2,i_j}\\}) \\\\[10pt]\n&\\ \\textbf{Model for probability of each option} \\\\\n\\{\\mu_{1,i_j}, \\mu_{2,i_j}, \\mu_{3,i_j}\\} =&\\ \\beta_{0_j} + \\beta_{1_j} \\text{Price[\\$3]}_{i_j} + \\beta_{2_j} \\text{Price[\\$4]}_{i_j} + \\\\\n&\\ \\beta_{3_j} \\text{Packaging[Plastic + sticker]}_{i_j} + \\beta_{4_j} \\text{Flavor[Nuts]}_{i_j} \\\\[20pt]  \n&\\ \\textbf{Respondent-specific slopes} \\\\\n\\left(\n  \\begin{array}{c}\n    \\begin{aligned}\n      &\\beta_{0_j} \\\\\n      &\\beta_{1_j} \\\\\n      &\\beta_{2_j} \\\\\n      &\\beta_{3_j} \\\\\n      &\\beta_{4_j}\n    \\end{aligned}\n  \\end{array}\n\\right) \\sim&\\ \\operatorname{Multivariate}\\ \\mathcal{N} \\left[\n\\left(\n  \\begin{array}{c}\n    \\begin{aligned}\n      &\\gamma^{\\beta_{0}}_{0} \\\\\n      &\\gamma^{\\beta_{1}}_{0} \\\\\n      &\\gamma^{\\beta_{2}}_{0} \\\\\n      &\\gamma^{\\beta_{3}}_{0} \\\\\n      &\\gamma^{\\beta_{4}}_{0}\n    \\end{aligned}\n  \\end{array}\n\\right)\n,\n\\left(\n  \\begin{array}{ccccc}\n     \\sigma^2_{\\beta_{0j}} & \\rho_{\\beta_{0j}\\beta_{1j}} & \\rho_{\\beta_{0j}\\beta_{2j}} & \\rho_{\\beta_{0j}\\beta_{3j}} & \\rho_{\\beta_{0j}\\beta_{4j}} \\\\\n     \\dots & \\sigma^2_{\\beta_{1j}} & \\rho_{\\beta_{1j}\\beta_{2j}} & \\rho_{\\beta_{1j}\\beta_{3j}} & \\rho_{\\beta_{1j}\\beta_{4j}} \\\\\n     \\dots & \\dots & \\sigma^2_{\\beta_{2j}} & \\rho_{\\beta_{2j}\\beta_{3j}} & \\rho_{\\beta_{2j}\\beta_{4j}} \\\\\n     \\dots & \\dots & \\dots & \\sigma^2_{\\beta_{3j}} & \\rho_{\\beta_{3j}\\beta_{4j}} \\\\\n     \\dots & \\dots & \\dots & \\dots & \\sigma^2_{\\beta_{4j}}\n  \\end{array}\n\\right)\n\\right] \\\\[10pt]\n&\\ \\textbf{Priors} \\\\\n\\beta_{0 \\dots 4} \\sim&\\ \\mathcal{N} (0, 3) \\qquad\\qquad\\ [\\text{Prior for choice-level coefficients}] \\\\\n\\gamma^{\\beta_{0 \\dots 4}}_0 \\sim&\\ \\mathcal{N} (0, 3) \\qquad\\qquad\\ [\\text{Prior for individual-level coefficients}] \\\\\n\\sigma_{\\beta_{0 \\dots 4}} \\sim&\\ \\operatorname{Exponential}(1) \\qquad [\\text{Prior for between-respondent intercept and slope variability}] \\\\\n\\rho \\sim&\\ \\operatorname{LKJ}(1) \\qquad\\qquad [\\text{Prior for correlation between random slopes and intercepts}]\n\\end{aligned}\n\\]\n\n\nmodel_stickers_categorical_brms &lt;- brm(\n  bf(choice_alt ~ 0 + price + packaging + flavor + (1 | ID | resp_id)),\n  data = stickers_choice_alt,\n  family = categorical(refcat = \"0\"),\n  prior = c(\n    prior(normal(0, 3), class = b, dpar = mu1),\n    prior(normal(0, 3), class = b, dpar = mu2),\n    prior(exponential(1), class = sd, dpar = mu1),\n    prior(exponential(1), class = sd, dpar = mu2)\n  ),\n  chains = 4, cores = 4, iter = 2000, seed = 1234,\n  backend = \"cmdstanr\", threads = threading(2),\n  file = \"models/model_stickers_categorical_brms\"\n)\n\n\nmodel_parameters(model_stickers_categorical_brms)\n\nParameter                    | Median |         95% CI |     pd |  Rhat |     ESS\n---------------------------------------------------------------------------------\nmu1_price$2                  |   0.75 | [ 0.60,  0.89] |   100% | 1.000 | 2211.00\nmu1_price$3                  |   0.02 | [-0.12,  0.15] | 58.53% | 1.000 | 2456.00\nmu1_price$4                  |  -0.86 | [-1.00, -0.73] |   100% | 1.001 | 2896.00\nmu1_packagingPlasticPsticker |   0.64 | [ 0.52,  0.77] |   100% | 1.001 | 2852.00\nmu1_flavorNuts               |  -1.93 | [-2.07, -1.80] |   100% | 1.000 | 2789.00\nmu2_price$2                  |   0.80 | [ 0.66,  0.94] |   100% | 1.000 | 2767.00\nmu2_price$3                  |   0.04 | [-0.10,  0.18] | 71.67% | 0.999 | 3099.00\nmu2_price$4                  |  -0.90 | [-1.04, -0.76] |   100% | 1.000 | 3147.00\nmu2_packagingPlasticPsticker |   0.57 | [ 0.45,  0.69] |   100% | 1.000 | 3132.00\nmu2_flavorNuts               |  -1.96 | [-2.09, -1.82] |   100% | 0.999 | 2917.00\n\n\n\nUncertainty intervals (equal-tailed) computed using a MCMC distribution approximation.\n\n\n\nThe model has a log- or logit-link. Consider using `exponentiate = TRUE` to interpret coefficients as ratios.\n\n\n\nstickers_cat_marginalized &lt;- model_stickers_categorical_brms |&gt;\n  gather_draws(`^b_.*$`, regex = TRUE) |&gt;\n  # Each variable name has \"mu1\", \"mu2\", etc. built in, like \"b_mu1_seat6\". This\n  # splits the .variable column into two parts based on a regular expression,\n  # creating one column for the mu part (\"b_mu1_\") and one for the rest of the\n  # variable name (\"seat6\")\n  separate_wider_regex(\n    .variable,\n    patterns = c(mu = \"b_mu\\\\d_\", .variable = \".*\")\n  ) |&gt;\n  # Find the average of the three mu estimates for each variable within each\n  # draw, or marginalize across the three options, since they're randomized\n  group_by(.variable, .draw) |&gt;\n  summarize(.value = mean(.value)) \n\n`summarise()` has grouped output by '.variable'. You can override using the `.groups` argument.\n\nstickers_cat_marginalized |&gt;\n  group_by(.variable) |&gt;\n  median_qi()\n\n# A tibble: 5 × 7\n  .variable                 .value  .lower .upper .width .point .interval\n  &lt;chr&gt;                      &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;  &lt;chr&gt;    \n1 flavorNuts               -1.94   -2.06   -1.83    0.95 median qi       \n2 packagingPlasticPsticker  0.606   0.500   0.712   0.95 median qi       \n3 price$2                   0.773   0.642   0.904   0.95 median qi       \n4 price$3                   0.0273 -0.0893  0.144   0.95 median qi       \n5 price$4                  -0.880  -0.995  -0.764   0.95 median qi       \n\n\n\nnewdata_all_combos &lt;- stickers |&gt; \n  tidyr::expand(price, packaging, flavor) |&gt; \n  mutate(resp_id = 4)\n\nall_preds_brms &lt;- model_stickers_categorical_brms |&gt; \n  epred_draws(newdata = newdata_all_combos) |&gt; \n  filter(.category == 0) |&gt; \n  mutate(.epred = 1 - .epred)\n\n\nall_preds_brms |&gt; \n  group_by(price, packaging, flavor) |&gt; \n  median_qi(.epred)\n\n# A tibble: 12 × 9\n   price packaging         flavor    .epred .lower .upper .width .point .interval\n   &lt;fct&gt; &lt;fct&gt;             &lt;fct&gt;      &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;  &lt;chr&gt;    \n 1 $2    Plastic + paper   Chocolate  0.813 0.789   0.834   0.95 median qi       \n 2 $2    Plastic + paper   Nuts       0.382 0.352   0.415   0.95 median qi       \n 3 $2    Plastic + sticker Chocolate  0.888 0.871   0.903   0.95 median qi       \n 4 $2    Plastic + sticker Nuts       0.532 0.498   0.565   0.95 median qi       \n 5 $3    Plastic + paper   Chocolate  0.673 0.643   0.703   0.95 median qi       \n 6 $3    Plastic + paper   Nuts       0.227 0.204   0.254   0.95 median qi       \n 7 $3    Plastic + sticker Chocolate  0.790 0.766   0.813   0.95 median qi       \n 8 $3    Plastic + sticker Nuts       0.351 0.318   0.383   0.95 median qi       \n 9 $4    Plastic + paper   Chocolate  0.454 0.421   0.489   0.95 median qi       \n10 $4    Plastic + paper   Nuts       0.106 0.0925  0.122   0.95 median qi       \n11 $4    Plastic + sticker Chocolate  0.604 0.570   0.638   0.95 median qi       \n12 $4    Plastic + sticker Nuts       0.179 0.158   0.202   0.95 median qi       \n\n\n\npreds_packaging_marginalized &lt;- all_preds_brms |&gt; \n  # Marginalize out the other covariates\n  group_by(packaging, .draw) |&gt;\n  summarize(avg = mean(.epred))\n\n`summarise()` has grouped output by 'packaging'. You can override using the `.groups` argument.\n\npreds_packaging_marginalized |&gt; \n  group_by(packaging) |&gt; \n  median_qi()\n\n# A tibble: 2 × 7\n  packaging           avg .lower .upper .width .point .interval\n  &lt;fct&gt;             &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;  &lt;chr&gt;    \n1 Plastic + paper   0.443  0.423  0.463   0.95 median qi       \n2 Plastic + sticker 0.558  0.537  0.577   0.95 median qi       \n\n\n\npreds_packaging_marginalized |&gt; \n  ggplot(aes(x = avg, y = packaging, fill = packaging)) +\n  stat_halfeye() +\n  geom_vline(xintercept = 0.5) +\n  scale_x_continuous(labels = label_percent()) +\n  guides(fill = \"none\")\n\n\n\n\n\n\n\n\n\npreds_packaging_marginalized |&gt;\n  compare_levels(variable = avg, by = packaging, comparison = \"control\") |&gt; \n  median_qi(avg)\n\n# A tibble: 1 × 7\n  packaging                             avg .lower .upper .width .point .interval\n  &lt;chr&gt;                               &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;  &lt;chr&gt;    \n1 Plastic + sticker - Plastic + paper 0.115 0.0949  0.135   0.95 median qi       \n\n\n\npreds_packaging_marginalized |&gt;\n  compare_levels(variable = avg, by = packaging, comparison = \"control\") |&gt; \n  ggplot(aes(x = avg, y = packaging)) +\n  stat_halfeye() +\n  geom_vline(xintercept = 0)",
    "crumbs": [
      "Analyzing: causal effects",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>MMs and AMCEs with Bayesian multinomial logistic regression</span>"
    ]
  },
  {
    "objectID": "utils-preds-freq.html",
    "href": "utils-preds-freq.html",
    "title": "10  Utilities and predictions with frequentist multinomial regression",
    "section": "",
    "text": "library(tidyverse)\nlibrary(mlogit)\nlibrary(marginaleffects)\nlibrary(parameters)\nlibrary(tinytable)\nlibrary(scales)\nlibrary(ggforce)\n\nstickers &lt;- readRDS(\"data/processed_data/study_5_sticker.rds\")",
    "crumbs": [
      "Analyzing: preference descriptions",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Utilities and predictions with frequentist multinomial regression</span>"
    ]
  },
  {
    "objectID": "utils-preds-bayes.html",
    "href": "utils-preds-bayes.html",
    "title": "11  Utilities and predictions with Bayesian multinomial regression",
    "section": "",
    "text": "11.1 Model\nmodel_stickers_mega_mlm_brms &lt;- brm(\n  bf(choice_alt ~\n    # Choice-level predictors that are nested within respondents...\n    (price + packaging + flavor) +\n    # ... with random respondent-specific slopes for the\n    # nested choice-level predictors\n    (1 + price + packaging + flavor | ID | resp_id)),\n  data = stickers_choice_alt,\n  family = categorical(refcat = \"0\"),\n  prior = c(\n    prior(normal(0, 3), class = b, dpar = mu1),\n    prior(normal(0, 3), class = b, dpar = mu2),\n    prior(exponential(1), class = sd, dpar = mu1),\n    prior(exponential(1), class = sd, dpar = mu2),\n    prior(lkj(1), class = cor)\n  ),\n  chains = 4, cores = 4, warmup = 1000, iter = 5000, seed = 1234,\n  backend = \"cmdstanr\", threads = threading(2), # refresh = 0,\n  control = list(adapt_delta = 0.9),\n  file = \"models/model_stickers_mega_mlm_brms\"\n)",
    "crumbs": [
      "Analyzing: preference descriptions",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Utilities and predictions with Bayesian multinomial regression</span>"
    ]
  },
  {
    "objectID": "utils-preds-bayes.html#part-worth-utilities-and-ratios",
    "href": "utils-preds-bayes.html#part-worth-utilities-and-ratios",
    "title": "11  Utilities and predictions with Bayesian multinomial regression",
    "section": "11.2 Part-worth utilities and ratios",
    "text": "11.2 Part-worth utilities and ratios\n\n11.2.1 Model βs\nThe coefficients from the model\n\nstickers_cat_marginalized &lt;- model_stickers_mega_mlm_brms %&gt;% \n  gather_draws(`b_.*`, regex = TRUE) %&gt;% \n  # Each variable name has \"mu1\", \"mu2\", etc. built in, like \"b_mu1_flavorNuts\". This\n  # splits the .variable column into two parts based on a regular expression,\n  # creating one column for the mu part (\"b_mu1_\") and one for the rest of the\n  # variable name (\"flavorNuts\")\n  separate_wider_regex(\n    .variable,\n    patterns = c(mu = \"b_mu\\\\d_\", .variable = \".*\")\n  ) %&gt;% \n  # Find the average of the two mu estimates for each variable within each\n  # draw, or marginalize across the two options, since they're randomized\n  group_by(.variable, .draw, .chain, .iteration) %&gt;% \n  summarize(.value = mean(.value)) \n\n`summarise()` has grouped output by '.variable', '.draw', '.chain'. You can override using the `.groups` argument.\n\nstickers_cat_marginalized |&gt; \n  filter(.variable != \"Intercept\") |&gt; \n  ggplot(aes(x = .value, y = .variable)) +\n  stat_halfeye() +\n  geom_vline(xintercept = 0)\n\n\n\n\n\n\n\n\n\nstickers_cat_marginalized |&gt; \n  group_by(.variable) |&gt; \n  median_hdi(.value)\n\n# A tibble: 5 × 7\n  .variable                .value .lower .upper .width .point .interval\n  &lt;chr&gt;                     &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;  &lt;chr&gt;    \n1 flavorNuts               -2.56  -2.87  -2.26    0.95 median hdi      \n2 Intercept                 1.25   1.01   1.49    0.95 median hdi      \n3 packagingPlasticPsticker  0.741  0.504  0.976   0.95 median hdi      \n4 price$3                  -1.05  -1.23  -0.887   0.95 median hdi      \n5 price$4                  -2.26  -2.49  -2.03    0.95 median hdi      \n\n\n\n\n11.2.2 Individual part-worths\n\npopulation_effects &lt;- stickers_cat_marginalized |&gt; \n  rename(value_population = .value)\n\npopulation_effects\n\n# A tibble: 80,000 × 5\n# Groups:   .variable, .draw, .chain [80,000]\n   .variable .draw .chain .iteration value_population\n   &lt;chr&gt;     &lt;int&gt;  &lt;int&gt;      &lt;int&gt;            &lt;dbl&gt;\n 1 Intercept     1      1          1             1.13\n 2 Intercept     2      1          2             1.47\n 3 Intercept     3      1          3             1.47\n 4 Intercept     4      1          4             1.40\n 5 Intercept     5      1          5             1.33\n 6 Intercept     6      1          6             1.35\n 7 Intercept     7      1          7             1.33\n 8 Intercept     8      1          8             1.18\n 9 Intercept     9      1          9             1.22\n10 Intercept    10      1         10             1.17\n# ℹ 79,990 more rows\n\n\n\nindividual_effects &lt;- model_stickers_mega_mlm_brms |&gt;\n  gather_draws(`r_.*`[resp_id,term], regex = TRUE) |&gt; \n  separate_wider_regex(\n    .variable,\n    patterns = c(mu = \"r_resp_id__mu\", .variable = \"\\\\d\")\n  ) |&gt; \n  group_by(resp_id, term, .chain, .iteration, .draw) |&gt; \n  summarize(.value = mean(.value)) \n\n\ncombined &lt;- individual_effects |&gt; \n  rename(.variable = term, value_individual = .value) |&gt; \n  left_join(population_effects, by = join_by(.variable, .chain, .iteration, .draw)) |&gt; \n  ungroup() |&gt; \n  filter(.variable != \"Intercept\") |&gt; \n  mutate(utility = value_individual + value_population)\n\n\npart_worths_posterior &lt;- combined |&gt; \n  group_by(resp_id, .variable) |&gt; \n  mean_hdi(utility) |&gt; \n  select(resp_id, .variable, utility) |&gt; \n  bind_rows(expand_grid(\n    utility = 0, \n    .variable = c(\"flavorChocolate\", \"packagingPackagingPpaper\", \"price$2\"), \n    resp_id = unique(combined$resp_id)\n  )) |&gt; \n  mutate(feature = case_when(\n    str_starts(.variable, \"price\") ~ \"Price\",\n    str_starts(.variable, \"packaging\") ~ \"Packaging\",\n    str_starts(.variable, \"flavor\") ~ \"Flavor\"\n  )) |&gt; \n  mutate(.variable = str_remove_all(.variable, \"^price|^packaging|^flavor\"))\n\nIndividual utilty part-worths:\n\npart_worths_posterior |&gt; \n  pivot_wider(names_from = c(feature, .variable), values_from = utility) |&gt; \n  slice(1:5) |&gt; \n  select(\n    ID = resp_id, `$2` = `Price_$2`, `$3` = `Price_$3`, `$4` = `Price_$4`,\n    Paper = Packaging_PackagingPpaper, Sticker = Packaging_PlasticPsticker,\n    Chocolate = Flavor_Chocolate, Nuts = Flavor_Nuts\n  ) |&gt; \n  tt() |&gt; \n  format_tt(j = 2:8, digits = 2, num_zero = TRUE, num_fmt = \"significant\") |&gt; \n  group_tt(\n    j = list(\n      \"Price\" = 2:4,\n      \"Packaging\" = 5:6,\n      \"Flavor\" = 7:8\n    )\n  ) |&gt; \n  style_tt(\n    i = 1:5,\n    j = c(2, 5, 7), line = \"l\"\n  ) |&gt; \n  style_tt(\n    i = 1, background = \"yellow\"\n  )\n\n\n\n    \n\n    \n    \n      \n        \n\n \nPrice\nPackaging\nFlavor\n\n        \n              \n                ID\n                $2\n                $3\n                $4\n                Paper\n                Sticker\n                Chocolate\n                Nuts\n              \n        \n        \n        \n                \n                  4\n                  0\n                  -0.82\n                  -1.6\n                  0\n                  -1.66\n                  0\n                  -4.109\n                \n                \n                  5\n                  0\n                  -1.42\n                  -3.3\n                  0\n                  0.27\n                  0\n                  -0.074\n                \n                \n                  6\n                  0\n                  -0.90\n                  -1.9\n                  0\n                  1.18\n                  0\n                  -3.275\n                \n                \n                  7\n                  0\n                  -1.00\n                  -1.6\n                  0\n                  1.20\n                  0\n                  -3.641\n                \n                \n                  8\n                  0\n                  -0.84\n                  -1.6\n                  0\n                  2.36\n                  0\n                  -3.557\n                \n        \n      \n    \n\n\n\nFor respondent 4, the difference in preference when moving from $2 to $4 is roughly the same as the preference for a sticker\nWe can also calculate the relative importance of each attribute for each individual by determining how much each attribute contributes to the overall utility of the choice. We first calculate the range of each\n\npart_worths_posterior |&gt; \n  filter(resp_id == 4) |&gt; \n  arrange(.variable) |&gt; \n  group_by(resp_id, feature) |&gt; \n  summarize(\n    range_text = glue::glue(\"{round(max(utility), 2)} − {round(min(utility), 2)}\"),\n    range = diff(range(utility))) |&gt; \n  mutate(pct_importance = range / sum(range)) |&gt; \n  ungroup() |&gt; \n  arrange(desc(feature)) |&gt; \n  janitor::adorn_totals() |&gt; \n  tt() |&gt; \n  format_tt(digits = 3, num_zero = TRUE, num_fmt = \"significant\") |&gt; \n  format_tt(j = 5, fn = scales::label_percent(accuracy = 0.1))\n\n`summarise()` has grouped output by 'resp_id'. You can override using the `.groups` argument.\n\n\n\n\n    \n\n    \n    \n      \n        \n        \n              \n                resp_id\n                feature\n                range_text\n                range\n                pct_importance\n              \n        \n        \n        \n                \n                  4\n                  Price\n                  0 − -1.61\n                  1.61\n                  21.8%\n                \n                \n                  4\n                  Packaging\n                  0 − -1.66\n                  1.66\n                  22.5%\n                \n                \n                  4\n                  Flavor\n                  0 − -4.11\n                  4.11\n                  55.7%\n                \n                \n                  Total\n                  -\n                  -\n                  7.37\n                  100.0%\n                \n        \n      \n    \n\n\n\n\npref_ranges_posterior &lt;- combined |&gt; \n  mutate(feature = case_when(\n    str_starts(.variable, \"price\") ~ \"Price\",\n    str_starts(.variable, \"packaging\") ~ \"Packaging\",\n    str_starts(.variable, \"flavor\") ~ \"Flavor\"\n  )) |&gt; \n  mutate(.variable = str_remove_all(.variable, \"^price|^packaging|^flavor\")) |&gt; \n  group_by(resp_id, feature, .draw) |&gt; \n  summarize(range = diff(range(c(0, utility)))) |&gt; \n  group_by(resp_id, .draw) |&gt; \n  mutate(pct_importance = range / sum(range))\n\n\nasdf &lt;- pref_ranges_posterior |&gt; \n  group_by(resp_id, feature) |&gt; \n  summarize(\n    range = mean(range),\n    relative_importance = mean(pct_importance)\n  ) |&gt; \n  filter(resp_id %in% 4:8) |&gt; \n  pivot_wider(names_from = feature, values_from = c(range, relative_importance))\n\n`summarise()` has grouped output by 'resp_id'. You can override using the `.groups` argument.\n\nasdf |&gt; \n  setNames(c(\"ID\", \"Flavor\", \"Packaging\", \"Price\", \"Flavor\", \"Packaging\", \"Price\")) |&gt; \n  tt() |&gt; \n  format_tt(j = 2:4, digits = 2, num_zero = TRUE, num_fmt = \"significant\") |&gt; \n  group_tt(\n    j = list(\n      \"Range\" = 2:4,\n      \"Importance\" = 5:7\n    )\n  ) |&gt; \n  style_tt(\n    i = 1:5,\n    j = c(2, 5), line = \"l\"\n  ) |&gt; \n  format_tt(j = 5:7, fn = scales::label_percent(accuracy = 0.1))\n\n\n\n    \n\n    \n    \n      \n        \n\n \nRange\nImportance\n\n        \n              \n                ID\n                Flavor\n                Packaging\n                Price\n                Flavor\n                Packaging\n                Price\n              \n        \n        \n        \n                \n                  4\n                  4.11\n                  1.70\n                  1.7\n                  55.7%\n                  21.6%\n                  22.7%\n                \n                \n                  5\n                  0.74\n                  0.71\n                  3.3\n                  15.0%\n                  14.4%\n                  70.6%\n                \n                \n                  6\n                  3.28\n                  1.28\n                  2.0\n                  50.4%\n                  18.9%\n                  30.7%\n                \n                \n                  7\n                  3.64\n                  1.28\n                  1.7\n                  55.3%\n                  18.8%\n                  26.0%\n                \n                \n                  8\n                  3.56\n                  2.36\n                  1.6\n                  47.0%\n                  30.7%\n                  22.3%\n                \n        \n      \n    \n\n\n\nFinally, we can aggregate these individual importance ratios into overall averages:\n\npref_ranges_posterior |&gt; \n  group_by(feature, .draw) |&gt; \n  summarize(relative_importance = mean(pct_importance)) |&gt; \n  median_hdi(relative_importance)\n\n`summarise()` has grouped output by 'feature'. You can override using the `.groups` argument.\n\n\n# A tibble: 3 × 7\n  feature   relative_importance .lower .upper .width .point .interval\n  &lt;chr&gt;                   &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;  &lt;chr&gt;    \n1 Flavor                  0.421  0.406  0.436   0.95 median hdi      \n2 Packaging               0.211  0.197  0.226   0.95 median hdi      \n3 Price                   0.367  0.350  0.385   0.95 median hdi      \n\n\n\npref_ranges_posterior |&gt; \n  group_by(feature, .draw) |&gt; \n  summarize(relative_importance = mean(pct_importance)) |&gt; \n  ungroup() |&gt; \n  mutate(feature = fct_reorder(feature, relative_importance)) |&gt; \n  ggplot(aes(x = relative_importance, y = feature)) +\n  stat_ccdfinterval(aes(fill = feature)) +\n  # stat_ccdfinterval(aes(fill = feature, slab_alpha = after_stat(f)),\n  #   thickness = 1, fill_type = \"gradient\"\n  # ) +\n  expand_limits(x = 0) +\n  scale_x_continuous(labels = scales::label_percent()) +\n  guides(fill = \"none\")\n\n`summarise()` has grouped output by 'feature'. You can override using the `.groups` argument.",
    "crumbs": [
      "Analyzing: preference descriptions",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Utilities and predictions with Bayesian multinomial regression</span>"
    ]
  },
  {
    "objectID": "utils-preds-bayes.html#predictions-and-transformations",
    "href": "utils-preds-bayes.html#predictions-and-transformations",
    "title": "11  Utilities and predictions with Bayesian multinomial regression",
    "section": "11.3 Predictions and transformations",
    "text": "11.3 Predictions and transformations\n\n11.3.1 Willingness-to-pay\n\n\n11.3.2 Simulated choice shares",
    "crumbs": [
      "Analyzing: preference descriptions",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Utilities and predictions with Bayesian multinomial regression</span>"
    ]
  },
  {
    "objectID": "utils-preds-bayes.html#market-simulations",
    "href": "utils-preds-bayes.html#market-simulations",
    "title": "11  Utilities and predictions with Bayesian multinomial regression",
    "section": "11.4 Market simulations?",
    "text": "11.4 Market simulations?",
    "crumbs": [
      "Analyzing: preference descriptions",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Utilities and predictions with Bayesian multinomial regression</span>"
    ]
  }
]